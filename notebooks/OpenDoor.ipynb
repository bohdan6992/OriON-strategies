{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f198af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-01\"  # papermill replacement\n",
    "import os\n",
    "output_dir = os.environ.get(\"ORION_SIGNALS_DIR\", \"../signals\")\n",
    "config_path = os.environ.get(\"DATUM_API_CONFIG_PATH\", \"../ops/datum_api_config.json\")\n",
    "dry_run = False\n",
    "\n",
    "# ensure output exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b328423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import pandas as pd\n",
    "from datum_api_client import DatumApi\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pip install xlrd\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import time, datetime, date, timedelta\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _add_minutes_to_time(t: time, minutes: int) -> time:\n",
    "    base = datetime.combine(date(2000, 1, 1), t)\n",
    "    return (base + timedelta(minutes=minutes)).time()\n",
    "\n",
    "\n",
    "def _time_bucket_str(t: time, bucket_minutes: int) -> str:\n",
    "    if bucket_minutes <= 1:\n",
    "        return f\"{t.hour:02d}:{t.minute:02d}\"\n",
    "    m = (t.minute // bucket_minutes) * bucket_minutes\n",
    "    return f\"{t.hour:02d}:{m:02d}\"\n",
    "\n",
    "\n",
    "def _linspace_edges(x: np.ndarray, n_bins: int) -> Optional[np.ndarray]:\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0:\n",
    "        return None\n",
    "    mn = float(x.min())\n",
    "    mx = float(x.max())\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx):\n",
    "        return None\n",
    "    if mn == mx:\n",
    "        mx = mn + 1e-12\n",
    "    return np.linspace(mn, mx, n_bins + 1, dtype=float)\n",
    "\n",
    "\n",
    "def _bin_index(x: np.ndarray, edges: np.ndarray) -> np.ndarray:\n",
    "    idx = np.digitize(x, edges, right=False) - 1\n",
    "    n_bins = edges.size - 1\n",
    "    return np.clip(idx, 0, n_bins - 1)\n",
    "\n",
    "\n",
    "def _merge_adjacent_ranges(ranges: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    if not ranges:\n",
    "        return []\n",
    "    ranges = sorted(ranges, key=lambda r: (r[\"from\"], r[\"to\"]))\n",
    "    out = []\n",
    "    cur = dict(ranges[0])\n",
    "    cur_sum = cur[\"rate\"] * cur[\"total\"]\n",
    "\n",
    "    for r in ranges[1:]:\n",
    "        if r[\"from\"] <= cur[\"to\"]:\n",
    "            cur[\"to\"] = max(cur[\"to\"], r[\"to\"])\n",
    "            cur_sum += r[\"rate\"] * r[\"total\"]\n",
    "            cur[\"total\"] += r[\"total\"]\n",
    "            cur[\"rate\"] = (cur_sum / cur[\"total\"]) if cur[\"total\"] else 0.0\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            cur = dict(r)\n",
    "            cur_sum = cur[\"rate\"] * cur[\"total\"]\n",
    "\n",
    "    out.append(cur)\n",
    "    for r in out:\n",
    "        r[\"from\"] = float(r[\"from\"])\n",
    "        r[\"to\"] = float(r[\"to\"])\n",
    "        r[\"rate\"] = float(r[\"rate\"])\n",
    "        r[\"total\"] = int(r[\"total\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _bins_1d_fast(\n",
    "    x: np.ndarray,\n",
    "    move: np.ndarray,\n",
    "    stack_delta: np.ndarray,\n",
    "    is_up: np.ndarray,\n",
    "    edges: np.ndarray,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"edges\": [...],\n",
    "        \"bins\": [\n",
    "          {\n",
    "            \"from\",\"to\",\"total\",\"up_count\",\"down_count\",\"up_rate\",\"down_rate\",\n",
    "            \"up\":   {\"n\",\"mean\",\"median\"},\n",
    "            \"down\": {\"n\",\"mean\",\"median\"},\n",
    "            \"stack_delta\": {\n",
    "              \"up\":   {\"n\",\"sum\",\"mean\",\"median\"},\n",
    "              \"down\": {\"n\",\"sum\",\"mean\",\"median\"}\n",
    "            }\n",
    "          }, ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    n_bins = edges.size - 1\n",
    "    b = _bin_index(x, edges)\n",
    "\n",
    "    total = np.bincount(b, minlength=n_bins).astype(int)\n",
    "    up_count = np.bincount(b[is_up], minlength=n_bins).astype(int)\n",
    "    dn_count = (total - up_count).astype(int)\n",
    "\n",
    "    # move means\n",
    "    up_move_sum = np.bincount(b[is_up], weights=move[is_up], minlength=n_bins)\n",
    "    dn_move_sum = np.bincount(b[~is_up], weights=move[~is_up], minlength=n_bins)\n",
    "    up_move_mean = np.divide(up_move_sum, up_count, out=np.full(n_bins, np.nan), where=up_count > 0)\n",
    "    dn_move_mean = np.divide(dn_move_sum, dn_count, out=np.full(n_bins, np.nan), where=dn_count > 0)\n",
    "\n",
    "    # stack_delta sums/means\n",
    "    up_sd_sum = np.bincount(b[is_up], weights=stack_delta[is_up], minlength=n_bins)\n",
    "    dn_sd_sum = np.bincount(b[~is_up], weights=stack_delta[~is_up], minlength=n_bins)\n",
    "    up_sd_mean = np.divide(up_sd_sum, up_count, out=np.full(n_bins, np.nan), where=up_count > 0)\n",
    "    dn_sd_mean = np.divide(dn_sd_sum, dn_count, out=np.full(n_bins, np.nan), where=dn_count > 0)\n",
    "\n",
    "    # medians per bin (n_bins small -> ok)\n",
    "    up_move_median = [None] * n_bins\n",
    "    dn_move_median = [None] * n_bins\n",
    "    up_sd_median = [None] * n_bins\n",
    "    dn_sd_median = [None] * n_bins\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        if total[i] == 0:\n",
    "            continue\n",
    "        mask_i = (b == i)\n",
    "\n",
    "        mv_i = move[mask_i]\n",
    "        sd_i = stack_delta[mask_i]\n",
    "        up_i = is_up[mask_i]\n",
    "\n",
    "        mv_up = mv_i[up_i]\n",
    "        mv_dn = mv_i[~up_i]\n",
    "        sd_up = sd_i[up_i]\n",
    "        sd_dn = sd_i[~up_i]\n",
    "\n",
    "        if mv_up.size:\n",
    "            up_move_median[i] = float(np.median(mv_up))\n",
    "        if mv_dn.size:\n",
    "            dn_move_median[i] = float(np.median(mv_dn))\n",
    "        if sd_up.size:\n",
    "            up_sd_median[i] = float(np.median(sd_up))\n",
    "        if sd_dn.size:\n",
    "            dn_sd_median[i] = float(np.median(sd_dn))\n",
    "\n",
    "    bins_out = []\n",
    "    for i in range(n_bins):\n",
    "        if total[i] == 0:\n",
    "            continue\n",
    "        up_rate = float(up_count[i] / total[i]) if total[i] else None\n",
    "        dn_rate = float(dn_count[i] / total[i]) if total[i] else None\n",
    "\n",
    "        bins_out.append({\n",
    "            \"from\": float(edges[i]),\n",
    "            \"to\": float(edges[i + 1]),\n",
    "            \"total\": int(total[i]),\n",
    "            \"up_count\": int(up_count[i]),\n",
    "            \"down_count\": int(dn_count[i]),\n",
    "            \"up_rate\": up_rate,\n",
    "            \"down_rate\": dn_rate,\n",
    "            \"up\": {\n",
    "                \"n\": int(up_count[i]),\n",
    "                \"mean\": None if not np.isfinite(up_move_mean[i]) else float(up_move_mean[i]),\n",
    "                \"median\": up_move_median[i],\n",
    "            },\n",
    "            \"down\": {\n",
    "                \"n\": int(dn_count[i]),\n",
    "                \"mean\": None if not np.isfinite(dn_move_mean[i]) else float(dn_move_mean[i]),\n",
    "                \"median\": dn_move_median[i],\n",
    "            },\n",
    "            \"stack_delta\": {\n",
    "                \"up\": {\n",
    "                    \"n\": int(up_count[i]),\n",
    "                    \"sum\": float(up_sd_sum[i]) if up_count[i] else 0.0,\n",
    "                    \"mean\": None if not np.isfinite(up_sd_mean[i]) else float(up_sd_mean[i]),\n",
    "                    \"median\": up_sd_median[i],\n",
    "                },\n",
    "                \"down\": {\n",
    "                    \"n\": int(dn_count[i]),\n",
    "                    \"sum\": float(dn_sd_sum[i]) if dn_count[i] else 0.0,\n",
    "                    \"mean\": None if not np.isfinite(dn_sd_mean[i]) else float(dn_sd_mean[i]),\n",
    "                    \"median\": dn_sd_median[i],\n",
    "                },\n",
    "            },\n",
    "        })\n",
    "\n",
    "    return {\"edges\": edges.tolist(), \"bins\": bins_out}\n",
    "\n",
    "\n",
    "def _heatmap_avg_fast(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    move: np.ndarray,\n",
    "    stack_delta: np.ndarray,\n",
    "    n_bins: int,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"x_edges\":[...], \"y_edges\":[...],\n",
    "        \"cells\":[\n",
    "          {\"x_from\",\"x_to\",\"y_from\",\"y_to\",\"n\",\"avg_move\",\"avg_stack_delta\"}\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    x_edges = _linspace_edges(x, n_bins)\n",
    "    y_edges = _linspace_edges(y, n_bins)\n",
    "    if x_edges is None or y_edges is None:\n",
    "        return {\"x_edges\": None, \"y_edges\": None, \"cells\": []}\n",
    "\n",
    "    xi = _bin_index(x, x_edges)\n",
    "    yi = _bin_index(y, y_edges)\n",
    "\n",
    "    cell = xi * n_bins + yi\n",
    "    counts = np.bincount(cell, minlength=n_bins * n_bins).astype(int)\n",
    "    move_sums = np.bincount(cell, weights=move, minlength=n_bins * n_bins)\n",
    "    sd_sums = np.bincount(cell, weights=stack_delta, minlength=n_bins * n_bins)\n",
    "\n",
    "    nonzero = np.nonzero(counts)[0]\n",
    "    cells = []\n",
    "    for k in nonzero:\n",
    "        n = int(counts[k])\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        bx = int(k // n_bins)\n",
    "        by = int(k % n_bins)\n",
    "        cells.append({\n",
    "            \"x_from\": float(x_edges[bx]),\n",
    "            \"x_to\": float(x_edges[bx + 1]),\n",
    "            \"y_from\": float(y_edges[by]),\n",
    "            \"y_to\": float(y_edges[by + 1]),\n",
    "            \"n\": n,\n",
    "            \"avg_move\": float(move_sums[k] / n),\n",
    "            \"avg_stack_delta\": float(sd_sums[k] / n),\n",
    "        })\n",
    "\n",
    "    return {\"x_edges\": x_edges.tolist(), \"y_edges\": y_edges.tolist(), \"cells\": cells}\n",
    "\n",
    "\n",
    "def analyze_open_strategy_fast(\n",
    "    input_parquet: str,\n",
    "    output_dir: str,\n",
    "    *,\n",
    "    time_col: str = \"dt\",\n",
    "    ticker_col: str = \"ticker\",\n",
    "    price_col: str = \"c\",\n",
    "    stack_col: str = \"Stack%\",\n",
    "    bench_col: str = \"Bench%\",\n",
    "    devsig_col: str = \"dev_sig\",\n",
    "    pre_from: time = time(9, 25),\n",
    "    pre_to: time = time(9, 30),\n",
    "    open_from: time = time(9, 30),\n",
    "    open_to: time = time(10, 0),\n",
    "    class_minutes: Tuple[int, ...] = (5, 10, 15, 20, 30),\n",
    "    end_tolerance_minutes: int = 3,\n",
    "    min_move_abs: float = 0.3,\n",
    "    n_bins_1d: int = 20,\n",
    "    n_bins_2d: int = 20,\n",
    "    minRateTop: float = 0.6,\n",
    "    minTotalTop: int = 4,\n",
    "    peak_time_bin_minutes: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fast OPEN exporter with all data needed for UI rendering:\n",
    "      - per class: 1D bins (edges+bins) for stack/bench/dev_sig with direction rates + move stats + stack_delta stats\n",
    "      - per class: 2D heatmaps (x_edges,y_edges,cells) with avg_move + avg_stack_delta\n",
    "      - for glob class: peak-time histogram of Stack% max/min during 09:30-10:00\n",
    "    Outputs:\n",
    "      - summary.csv\n",
    "      - onefile.jsonl\n",
    "      - best_params.jsonl  (✅ UPDATED: matches OpenDoorBestParamsPath reader)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    summary_path = os.path.join(output_dir, \"summary.csv\")\n",
    "    onefile_path = os.path.join(output_dir, \"onefile.jsonl\")\n",
    "    best_params_path = os.path.join(output_dir, \"best_params.jsonl\")\n",
    "\n",
    "    # 1) Read minimal columns\n",
    "    cols = [time_col, ticker_col, price_col, stack_col, bench_col, devsig_col]\n",
    "    df = pd.read_parquet(input_parquet, columns=cols)\n",
    "\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in input: {missing}\")\n",
    "\n",
    "    df = df[df[time_col].notna() & df[ticker_col].notna()].copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df[\"session_date\"] = df[time_col].dt.date\n",
    "    df[\"time_only\"] = df[time_col].dt.time\n",
    "\n",
    "    for c in (price_col, stack_col, bench_col, devsig_col):\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df[df[price_col].notna()].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows with price present after cleaning.\")\n",
    "\n",
    "    # 2) Start (pre-window): first available per (ticker,date) WITH non-NaN features\n",
    "    pre_mask = (df[\"time_only\"] >= pre_from) & (df[\"time_only\"] <= pre_to)\n",
    "    df_pre = df.loc[pre_mask].copy()\n",
    "    df_pre = df_pre[df_pre[stack_col].notna() & df_pre[bench_col].notna() & df_pre[devsig_col].notna()].copy()\n",
    "    if df_pre.empty:\n",
    "        raise ValueError(\"No pre-window rows with non-NaN Stack%/Bench%/dev_sig in 09:25–09:30.\")\n",
    "\n",
    "    df_pre = df_pre.sort_values(time_col)\n",
    "    start_idx = df_pre.groupby([ticker_col, \"session_date\"])[time_col].idxmin()\n",
    "    starts = df_pre.loc[start_idx].copy()\n",
    "\n",
    "    starts[\"__key\"] = starts[ticker_col].astype(str) + \"|\" + starts[\"session_date\"].astype(str)\n",
    "    starts = starts.set_index(\"__key\", drop=True)\n",
    "\n",
    "    # 3) Prepare end candidates once: only 09:30..10:00 + tolerance\n",
    "    open_end_to = _add_minutes_to_time(open_to, end_tolerance_minutes)\n",
    "    df_open = df[(df[\"time_only\"] >= open_from) & (df[\"time_only\"] <= open_end_to)].copy()\n",
    "    if df_open.empty:\n",
    "        raise ValueError(\"No rows in open window 09:30–10:00(+tolerance).\")\n",
    "\n",
    "    df_open[\"__key\"] = df_open[ticker_col].astype(str) + \"|\" + df_open[\"session_date\"].astype(str)\n",
    "\n",
    "    # Define classes: glob + Nm\n",
    "    class_defs: List[Tuple[str, time]] = [(\"glob\", open_to)]\n",
    "    for m in class_minutes:\n",
    "        class_defs.append((f\"{int(m)}m\", _add_minutes_to_time(open_from, int(m))))\n",
    "\n",
    "    events_all = []\n",
    "\n",
    "    # 4) Build events per class\n",
    "    for cls, t_target in class_defs:\n",
    "        t_end_to = _add_minutes_to_time(t_target, end_tolerance_minutes)\n",
    "        m_end = (df_open[\"time_only\"] >= t_target) & (df_open[\"time_only\"] <= t_end_to)\n",
    "        cand = df_open.loc[m_end].copy()\n",
    "        if cand.empty:\n",
    "            continue\n",
    "\n",
    "        cand = cand.sort_values(time_col)\n",
    "        end_idx = cand.groupby([ticker_col, \"session_date\"])[time_col].idxmin()\n",
    "        ends = cand.loc[end_idx].copy()\n",
    "\n",
    "        ends[\"__key\"] = ends[ticker_col].astype(str) + \"|\" + ends[\"session_date\"].astype(str)\n",
    "        ends = ends.set_index(\"__key\", drop=True)\n",
    "\n",
    "        common_keys = starts.index.intersection(ends.index)\n",
    "        if common_keys.empty:\n",
    "            continue\n",
    "\n",
    "        s = starts.loc[common_keys]\n",
    "        e = ends.loc[common_keys]\n",
    "\n",
    "        # chronological check\n",
    "        s_dt = s[time_col].to_numpy()\n",
    "        e_dt = e[time_col].to_numpy()\n",
    "        ok = e_dt > s_dt\n",
    "        if not ok.any():\n",
    "            continue\n",
    "\n",
    "        idx_ok = np.where(ok)[0]\n",
    "        s = s.iloc[idx_ok]\n",
    "        e = e.iloc[idx_ok]\n",
    "\n",
    "        start_price = s[price_col].to_numpy(dtype=float)\n",
    "        end_price = e[price_col].to_numpy(dtype=float)\n",
    "        move_pct = 100.0 * (end_price / start_price - 1.0)\n",
    "\n",
    "        # stack_delta (end_stack - start_stack)\n",
    "        start_stack = s[stack_col].to_numpy(dtype=float)\n",
    "        end_stack = e[stack_col].to_numpy(dtype=float)\n",
    "        stack_delta = end_stack - start_stack\n",
    "\n",
    "        # filter by min_move_abs\n",
    "        good = np.isfinite(move_pct) & (np.abs(move_pct) >= float(min_move_abs))\n",
    "        if not good.any():\n",
    "            continue\n",
    "\n",
    "        idx_good = np.where(good)[0]\n",
    "        s = s.iloc[idx_good]\n",
    "        e = e.iloc[idx_good]\n",
    "\n",
    "        move_pct = move_pct[good]\n",
    "        stack_delta = stack_delta[good]\n",
    "        start_price = start_price[good]\n",
    "        end_price = end_price[good]\n",
    "\n",
    "        dir_up = move_pct > 0.0\n",
    "        direction = np.where(dir_up, \"up\", \"down\")\n",
    "\n",
    "        ev = pd.DataFrame({\n",
    "            ticker_col: s[ticker_col].to_numpy(),\n",
    "            \"session_date\": s[\"session_date\"].to_numpy(),\n",
    "            \"class\": cls,\n",
    "            \"start_dt\": s[time_col].to_numpy(),\n",
    "            \"end_dt\": e[time_col].to_numpy(),\n",
    "            \"x_stack\": s[stack_col].to_numpy(dtype=float),\n",
    "            \"x_bench\": s[bench_col].to_numpy(dtype=float),\n",
    "            \"x_dev\": s[devsig_col].to_numpy(dtype=float),\n",
    "            \"start_price\": start_price,\n",
    "            \"end_price\": end_price,\n",
    "            \"move_pct\": move_pct,\n",
    "            \"stack_delta\": stack_delta,\n",
    "            \"dir\": direction,\n",
    "        })\n",
    "\n",
    "        # drop rows where any needed feature/outcome is NaN\n",
    "        ev = ev[\n",
    "            np.isfinite(ev[\"x_stack\"])\n",
    "            & np.isfinite(ev[\"x_bench\"])\n",
    "            & np.isfinite(ev[\"x_dev\"])\n",
    "            & np.isfinite(ev[\"move_pct\"])\n",
    "            & np.isfinite(ev[\"stack_delta\"])\n",
    "        ].copy()\n",
    "\n",
    "        if not ev.empty:\n",
    "            events_all.append(ev)\n",
    "\n",
    "    if not events_all:\n",
    "        raise ValueError(\"No events built for any class after filters. Check min_move_abs / data coverage.\")\n",
    "\n",
    "    events = pd.concat(events_all, ignore_index=True)\n",
    "\n",
    "    # 5) summary.csv (wide, one row per ticker)\n",
    "    rows = []\n",
    "    for cls, _ in class_defs:\n",
    "        evc = events[events[\"class\"] == cls]\n",
    "        if evc.empty:\n",
    "            continue\n",
    "\n",
    "        g = evc.groupby(ticker_col)\n",
    "        tmp = g.agg(\n",
    "            total=(\"dir\", \"count\"),\n",
    "            up=(\"dir\", lambda s: int((s == \"up\").sum())),\n",
    "            down=(\"dir\", lambda s: int((s == \"down\").sum())),\n",
    "            mean_move=(\"move_pct\", \"mean\"),\n",
    "            median_move=(\"move_pct\", \"median\"),\n",
    "            mean_stack_delta=(\"stack_delta\", \"mean\"),\n",
    "            median_stack_delta=(\"stack_delta\", \"median\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        tmp[f\"open_{cls}_total\"] = tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_up_rate\"] = tmp[\"up\"] / tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_down_rate\"] = tmp[\"down\"] / tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_mean_move\"] = tmp[\"mean_move\"]\n",
    "        tmp[f\"open_{cls}_median_move\"] = tmp[\"median_move\"]\n",
    "        tmp[f\"open_{cls}_mean_stack_delta\"] = tmp[\"mean_stack_delta\"]\n",
    "        tmp[f\"open_{cls}_median_stack_delta\"] = tmp[\"median_stack_delta\"]\n",
    "\n",
    "        tmp = tmp[\n",
    "            [\n",
    "                ticker_col,\n",
    "                f\"open_{cls}_total\",\n",
    "                f\"open_{cls}_up_rate\",\n",
    "                f\"open_{cls}_down_rate\",\n",
    "                f\"open_{cls}_mean_move\",\n",
    "                f\"open_{cls}_median_move\",\n",
    "                f\"open_{cls}_mean_stack_delta\",\n",
    "                f\"open_{cls}_median_stack_delta\",\n",
    "            ]\n",
    "        ]\n",
    "        rows.append(tmp)\n",
    "\n",
    "    summary = rows[0]\n",
    "    for r in rows[1:]:\n",
    "        summary = summary.merge(r, on=ticker_col, how=\"outer\")\n",
    "\n",
    "    summary = summary.sort_values(ticker_col)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "\n",
    "    # 6) onefile.jsonl + best_params.jsonl\n",
    "    with open(onefile_path, \"w\", encoding=\"utf-8\") as onefile_out, open(best_params_path, \"w\", encoding=\"utf-8\") as best_out:\n",
    "\n",
    "        # peak histogram source (strict 09:30..10:00) with Stack% present\n",
    "        df_peaks = df[\n",
    "            (df[\"time_only\"] >= open_from)\n",
    "            & (df[\"time_only\"] <= open_to)\n",
    "            & df[stack_col].notna()\n",
    "        ].copy()\n",
    "\n",
    "        for ticker, ev_t in events.groupby(ticker_col):\n",
    "            rec = {\"ticker\": ticker, \"classes\": {}}\n",
    "\n",
    "            # ✅ best_params row (matches StrategySignalService.LoadOpenDoorBestParams)\n",
    "            best = {\n",
    "                \"ticker\": ticker,\n",
    "                \"ratings\": {},     # cls -> { any:{rate,total}, up:{rate,total}, down:{rate,total} }\n",
    "                \"best_ranges\": {}, # cls -> { stack:{up/down}, bench:{...}, dev_sig:{...} }\n",
    "            }\n",
    "\n",
    "            # glob peak-time histogram per ticker\n",
    "            up_counts: Dict[str, int] = {}\n",
    "            down_counts: Dict[str, int] = {}\n",
    "            w = df_peaks[df_peaks[ticker_col] == ticker]\n",
    "            if not w.empty:\n",
    "                for _, gd in w.groupby(\"session_date\"):\n",
    "                    i_max = gd[stack_col].idxmax()\n",
    "                    i_min = gd[stack_col].idxmin()\n",
    "                    t_max = gd.loc[i_max, \"time_only\"]\n",
    "                    t_min = gd.loc[i_min, \"time_only\"]\n",
    "                    k_up = _time_bucket_str(t_max, peak_time_bin_minutes)\n",
    "                    k_dn = _time_bucket_str(t_min, peak_time_bin_minutes)\n",
    "                    up_counts[k_up] = up_counts.get(k_up, 0) + 1\n",
    "                    down_counts[k_dn] = down_counts.get(k_dn, 0) + 1\n",
    "\n",
    "            peak_hist = {\n",
    "                \"bin_minutes\": int(peak_time_bin_minutes),\n",
    "                \"up_peak\": up_counts,\n",
    "                \"down_peak\": down_counts\n",
    "            }\n",
    "\n",
    "            for cls, evc in ev_t.groupby(\"class\"):\n",
    "                mv = evc[\"move_pct\"].to_numpy(dtype=float)\n",
    "                sd = evc[\"stack_delta\"].to_numpy(dtype=float)\n",
    "\n",
    "                x_stack = evc[\"x_stack\"].to_numpy(dtype=float)\n",
    "                x_bench = evc[\"x_bench\"].to_numpy(dtype=float)\n",
    "                x_dev = evc[\"x_dev\"].to_numpy(dtype=float)\n",
    "\n",
    "                is_up = (evc[\"dir\"].to_numpy() == \"up\")\n",
    "\n",
    "                cls_obj = {\n",
    "                    \"stats\": {\n",
    "                        \"total\": int(evc.shape[0]),\n",
    "                        \"up_count\": int(is_up.sum()),\n",
    "                        \"down_count\": int((~is_up).sum()),\n",
    "                        \"up_rate\": float(is_up.mean()) if evc.shape[0] else None,\n",
    "                        \"down_rate\": float((~is_up).mean()) if evc.shape[0] else None,\n",
    "                        \"mean_move\": float(np.mean(mv)) if mv.size else None,\n",
    "                        \"median_move\": float(np.median(mv)) if mv.size else None,\n",
    "                        \"mean_stack_delta\": float(np.mean(sd)) if sd.size else None,\n",
    "                        \"median_stack_delta\": float(np.median(sd)) if sd.size else None,\n",
    "                    },\n",
    "                    \"bins_1d\": {},\n",
    "                    \"heatmaps\": {},\n",
    "                }\n",
    "\n",
    "                # 1D bins\n",
    "                e_stack = _linspace_edges(x_stack, n_bins_1d)\n",
    "                e_bench = _linspace_edges(x_bench, n_bins_1d)\n",
    "                e_dev = _linspace_edges(x_dev, n_bins_1d)\n",
    "\n",
    "                cls_obj[\"bins_1d\"][\"stack\"] = {\"edges\": None, \"bins\": []} if e_stack is None else _bins_1d_fast(x_stack, mv, sd, is_up, e_stack)\n",
    "                cls_obj[\"bins_1d\"][\"bench\"] = {\"edges\": None, \"bins\": []} if e_bench is None else _bins_1d_fast(x_bench, mv, sd, is_up, e_bench)\n",
    "                cls_obj[\"bins_1d\"][\"dev_sig\"] = {\"edges\": None, \"bins\": []} if e_dev is None else _bins_1d_fast(x_dev, mv, sd, is_up, e_dev)\n",
    "\n",
    "                # 2D heatmaps\n",
    "                cls_obj[\"heatmaps\"][\"stack_vs_bench\"] = _heatmap_avg_fast(x_stack, x_bench, mv, sd, n_bins_2d)\n",
    "                cls_obj[\"heatmaps\"][\"stack_vs_dev\"] = _heatmap_avg_fast(x_stack, x_dev, mv, sd, n_bins_2d)\n",
    "                cls_obj[\"heatmaps\"][\"bench_vs_dev\"] = _heatmap_avg_fast(x_bench, x_dev, mv, sd, n_bins_2d)\n",
    "\n",
    "                # ============================\n",
    "                # ✅ best_params.jsonl payload\n",
    "                # ============================\n",
    "\n",
    "                total_cls = int(evc.shape[0])\n",
    "                up_rate_cls = float(is_up.mean()) if total_cls else 0.0\n",
    "                down_rate_cls = 1.0 - up_rate_cls if total_cls else 0.0\n",
    "\n",
    "                best[\"ratings\"][cls] = {\n",
    "                    \"any\":  {\"rate\": float(max(up_rate_cls, down_rate_cls)), \"total\": int(total_cls)},\n",
    "                    \"up\":   {\"rate\": float(up_rate_cls), \"total\": int(total_cls)},\n",
    "                    \"down\": {\"rate\": float(down_rate_cls), \"total\": int(total_cls)},\n",
    "                }\n",
    "\n",
    "                best_cls_ranges: Dict[str, Any] = {}\n",
    "                for feat in (\"stack\", \"bench\", \"dev_sig\"):\n",
    "                    bins_list = cls_obj[\"bins_1d\"][feat][\"bins\"]\n",
    "\n",
    "                    up_ranges = []\n",
    "                    dn_ranges = []\n",
    "\n",
    "                    for b in bins_list:\n",
    "                        total = int(b.get(\"total\", 0) or 0)\n",
    "                        if total < int(minTotalTop):\n",
    "                            continue\n",
    "\n",
    "                        upc = int(b.get(\"up_count\", 0) or 0)\n",
    "                        dnc = int(b.get(\"down_count\", 0) or 0)\n",
    "                        if total <= 0:\n",
    "                            continue\n",
    "\n",
    "                        ur = upc / total\n",
    "                        dr = dnc / total\n",
    "\n",
    "                        if ur >= float(minRateTop):\n",
    "                            up_ranges.append({\"from\": b[\"from\"], \"to\": b[\"to\"], \"rate\": float(ur), \"total\": int(total)})\n",
    "                        if dr >= float(minRateTop):\n",
    "                            dn_ranges.append({\"from\": b[\"from\"], \"to\": b[\"to\"], \"rate\": float(dr), \"total\": int(total)})\n",
    "\n",
    "                    best_cls_ranges[feat] = {\n",
    "                        \"up\": _merge_adjacent_ranges(up_ranges),\n",
    "                        \"down\": _merge_adjacent_ranges(dn_ranges),\n",
    "                    }\n",
    "\n",
    "                best[\"best_ranges\"][cls] = best_cls_ranges\n",
    "\n",
    "                if cls == \"glob\":\n",
    "                    cls_obj[\"glob_peak_time\"] = peak_hist\n",
    "\n",
    "                rec[\"classes\"][cls] = cls_obj\n",
    "\n",
    "            onefile_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            best_out.write(json.dumps(best, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"GOTOVO:\")\n",
    "    print(\" \", summary_path)\n",
    "    print(\" \", onefile_path)\n",
    "    print(\" \", best_params_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d69fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTOVO:\n",
      "  ARBITRAGE/open_analysis_fast\\summary.csv\n",
      "  ARBITRAGE/open_analysis_fast\\onefile.jsonl\n",
      "  ARBITRAGE/open_analysis_fast\\best_params.jsonl\n"
     ]
    }
   ],
   "source": [
    "# from datetime import time, datetime, date, timedelta\n",
    "\n",
    "# analyze_open_strategy_fast(\n",
    "#     input_parquet=\"ARBITRAGE/final_filtered.parquet\",\n",
    "#     output_dir=\"ARBITRAGE/open_analysis_fast\",\n",
    "#     n_bins_1d=10,\n",
    "#     n_bins_2d=10,\n",
    "#     min_move_abs=0.3,\n",
    "#     minRateTop=0.6,\n",
    "#     minTotalTop=4,\n",
    "#     peak_time_bin_minutes=1,\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
