{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f198af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-01\"  # papermill replacement\n",
    "import os\n",
    "output_dir = os.environ.get(\"ORION_SIGNALS_DIR\", \"../signals\")\n",
    "config_path = os.environ.get(\"DATUM_API_CONFIG_PATH\", \"../ops/datum_api_config.json\")\n",
    "dry_run = False\n",
    "\n",
    "# ensure output exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b328423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import pandas as pd\n",
    "from datum_api_client import DatumApi\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pip install xlrd\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from datetime import time, datetime, date, timedelta\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ====== HELPERS ==========\n",
    "# =========================\n",
    "\n",
    "def _add_minutes_to_time(t: time, minutes: int) -> time:\n",
    "    base = datetime.combine(date(2000, 1, 1), t)\n",
    "    return (base + timedelta(minutes=minutes)).time()\n",
    "\n",
    "\n",
    "def _time_bucket_str(t: time, bucket_minutes: int) -> str:\n",
    "    if bucket_minutes <= 1:\n",
    "        return f\"{t.hour:02d}:{t.minute:02d}\"\n",
    "    m = (t.minute // bucket_minutes) * bucket_minutes\n",
    "    return f\"{t.hour:02d}:{m:02d}\"\n",
    "\n",
    "\n",
    "def _linspace_edges(x: np.ndarray, n_bins: int) -> Optional[np.ndarray]:\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0:\n",
    "        return None\n",
    "    mn = float(x.min())\n",
    "    mx = float(x.max())\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx):\n",
    "        return None\n",
    "    if mn == mx:\n",
    "        mx = mn + 1e-12\n",
    "    return np.linspace(mn, mx, n_bins + 1, dtype=float)\n",
    "\n",
    "\n",
    "def _bin_index(x: np.ndarray, edges: np.ndarray) -> np.ndarray:\n",
    "    idx = np.digitize(x, edges, right=False) - 1\n",
    "    n_bins = edges.size - 1\n",
    "    return np.clip(idx, 0, n_bins - 1)\n",
    "\n",
    "\n",
    "def _merge_adjacent_ranges(ranges: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    if not ranges:\n",
    "        return []\n",
    "    ranges = sorted(ranges, key=lambda r: (r[\"from\"], r[\"to\"]))\n",
    "    out = []\n",
    "    cur = dict(ranges[0])\n",
    "    cur_sum = cur[\"rate\"] * cur[\"total\"]\n",
    "\n",
    "    for r in ranges[1:]:\n",
    "        if r[\"from\"] <= cur[\"to\"]:\n",
    "            cur[\"to\"] = max(cur[\"to\"], r[\"to\"])\n",
    "            cur_sum += r[\"rate\"] * r[\"total\"]\n",
    "            cur[\"total\"] += r[\"total\"]\n",
    "            cur[\"rate\"] = (cur_sum / cur[\"total\"]) if cur[\"total\"] else 0.0\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            cur = dict(r)\n",
    "            cur_sum = cur[\"rate\"] * cur[\"total\"]\n",
    "\n",
    "    out.append(cur)\n",
    "    for r in out:\n",
    "        r[\"from\"] = float(r[\"from\"])\n",
    "        r[\"to\"] = float(r[\"to\"])\n",
    "        r[\"rate\"] = float(r[\"rate\"])\n",
    "        r[\"total\"] = int(r[\"total\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ====== MAIN FUNC ========\n",
    "# =========================\n",
    "\n",
    "def analyze_open_strategy_fast(\n",
    "    input_parquet: str,\n",
    "    output_dir: str,\n",
    "    *,\n",
    "    time_col: str = \"dt\",\n",
    "    ticker_col: str = \"ticker\",\n",
    "    price_col: str = \"c\",\n",
    "    stack_col: str = \"Stack%\",\n",
    "    bench_col: str = \"Bench%\",\n",
    "    devsig_col: str = \"dev_sig\",\n",
    "    pre_from: time = time(9, 25),\n",
    "    pre_to: time = time(9, 30),\n",
    "    open_from: time = time(9, 30),\n",
    "    open_to: time = time(10, 0),\n",
    "    class_minutes: Tuple[int, ...] = (5, 10, 15, 20, 30),\n",
    "    end_tolerance_minutes: int = 3,\n",
    "    min_move_abs: float = 0.3,\n",
    "    n_bins_1d: int = 20,\n",
    "    n_bins_2d: int = 20,\n",
    "    minRateTop: float = 0.6,\n",
    "    minTotalTop: int = 4,\n",
    "    peak_time_bin_minutes: int = 1,\n",
    ") -> None:\n",
    "\n",
    "    # =========================\n",
    "    # ===== PATH SETUP ========\n",
    "    # =========================\n",
    "\n",
    "    out_dir = Path(output_dir).resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    summary_path = out_dir / \"summary.csv\"\n",
    "    onefile_path = out_dir / \"onefile.jsonl\"\n",
    "    best_params_path = out_dir / \"best_params.jsonl\"\n",
    "\n",
    "    def _open_text(path: Path, mode: str = \"wt\"):\n",
    "        if str(path).lower().endswith(\".gz\"):\n",
    "            return gzip.open(path, mode, encoding=\"utf-8\", newline=\"\\n\", compresslevel=6)\n",
    "        return open(path, mode.replace(\"t\", \"\"), encoding=\"utf-8\", newline=\"\\n\")\n",
    "\n",
    "    # =========================\n",
    "    # ===== INIT SUMMARY ======\n",
    "    # =========================\n",
    "\n",
    "    summary_cols = [ticker_col]\n",
    "    class_names = [\"glob\"] + [f\"{m}m\" for m in class_minutes]\n",
    "\n",
    "    for cls in class_names:\n",
    "        summary_cols.extend([\n",
    "            f\"open_{cls}_total\",\n",
    "            f\"open_{cls}_up_rate\",\n",
    "            f\"open_{cls}_down_rate\",\n",
    "            f\"open_{cls}_mean_move\",\n",
    "            f\"open_{cls}_median_move\",\n",
    "            f\"open_{cls}_mean_stack_delta\",\n",
    "            f\"open_{cls}_median_stack_delta\",\n",
    "        ])\n",
    "\n",
    "    pd.DataFrame(columns=summary_cols).to_csv(summary_path, index=False, mode=\"w\")\n",
    "\n",
    "    # =========================\n",
    "    # ===== INIT BEST PARAMS ==\n",
    "    # =========================\n",
    "\n",
    "    with _open_text(best_params_path, \"wt\") as best_out:\n",
    "        best_out.write(json.dumps({\n",
    "            \"meta\": {\n",
    "                \"version\": \"open_v2\",\n",
    "                \"generated_at\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # =========================\n",
    "    # ===== ORIGINAL LOGIC ====\n",
    "    # =========================\n",
    "\n",
    "    cols = [time_col, ticker_col, price_col, stack_col, bench_col, devsig_col]\n",
    "    df = pd.read_parquet(input_parquet, columns=cols)\n",
    "\n",
    "    df = df[df[time_col].notna() & df[ticker_col].notna()].copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df[\"session_date\"] = df[time_col].dt.date\n",
    "    df[\"time_only\"] = df[time_col].dt.time\n",
    "\n",
    "    for c in (price_col, stack_col, bench_col, devsig_col):\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df[df[price_col].notna()].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No rows with price present.\")\n",
    "\n",
    "    # --- PRE window\n",
    "    pre_mask = (df[\"time_only\"] >= pre_from) & (df[\"time_only\"] <= pre_to)\n",
    "    df_pre = df.loc[pre_mask].copy()\n",
    "    df_pre = df_pre[\n",
    "        df_pre[stack_col].notna() &\n",
    "        df_pre[bench_col].notna() &\n",
    "        df_pre[devsig_col].notna()\n",
    "    ].copy()\n",
    "\n",
    "    if df_pre.empty:\n",
    "        raise ValueError(\"No pre-window rows.\")\n",
    "\n",
    "    df_pre = df_pre.sort_values(time_col)\n",
    "    start_idx = df_pre.groupby([ticker_col, \"session_date\"])[time_col].idxmin()\n",
    "    starts = df_pre.loc[start_idx].copy()\n",
    "\n",
    "    starts[\"__key\"] = starts[ticker_col].astype(str) + \"|\" + starts[\"session_date\"].astype(str)\n",
    "    starts = starts.set_index(\"__key\", drop=True)\n",
    "\n",
    "    # --- OPEN window\n",
    "    open_end_to = _add_minutes_to_time(open_to, end_tolerance_minutes)\n",
    "    df_open = df[(df[\"time_only\"] >= open_from) & (df[\"time_only\"] <= open_end_to)].copy()\n",
    "    df_open[\"__key\"] = df_open[ticker_col].astype(str) + \"|\" + df_open[\"session_date\"].astype(str)\n",
    "\n",
    "    class_defs = [(\"glob\", open_to)]\n",
    "    for m in class_minutes:\n",
    "        class_defs.append((f\"{int(m)}m\", _add_minutes_to_time(open_from, int(m))))\n",
    "\n",
    "    events_all = []\n",
    "\n",
    "    for cls, t_target in class_defs:\n",
    "        t_end_to = _add_minutes_to_time(t_target, end_tolerance_minutes)\n",
    "        m_end = (df_open[\"time_only\"] >= t_target) & (df_open[\"time_only\"] <= t_end_to)\n",
    "        cand = df_open.loc[m_end].copy()\n",
    "        if cand.empty:\n",
    "            continue\n",
    "\n",
    "        cand = cand.sort_values(time_col)\n",
    "        end_idx = cand.groupby([ticker_col, \"session_date\"])[time_col].idxmin()\n",
    "        ends = cand.loc[end_idx].copy()\n",
    "\n",
    "        ends[\"__key\"] = ends[ticker_col].astype(str) + \"|\" + ends[\"session_date\"].astype(str)\n",
    "        ends = ends.set_index(\"__key\", drop=True)\n",
    "\n",
    "        common_keys = starts.index.intersection(ends.index)\n",
    "        if common_keys.empty:\n",
    "            continue\n",
    "\n",
    "        s = starts.loc[common_keys]\n",
    "        e = ends.loc[common_keys]\n",
    "\n",
    "        s_dt = s[time_col].to_numpy()\n",
    "        e_dt = e[time_col].to_numpy()\n",
    "        ok = e_dt > s_dt\n",
    "        if not ok.any():\n",
    "            continue\n",
    "\n",
    "        s = s.iloc[ok]\n",
    "        e = e.iloc[ok]\n",
    "\n",
    "        start_price = s[price_col].to_numpy(dtype=float)\n",
    "        end_price = e[price_col].to_numpy(dtype=float)\n",
    "        move_pct = 100.0 * (end_price / start_price - 1.0)\n",
    "\n",
    "        start_stack = s[stack_col].to_numpy(dtype=float)\n",
    "        end_stack = e[stack_col].to_numpy(dtype=float)\n",
    "        stack_delta = end_stack - start_stack\n",
    "\n",
    "        good = np.isfinite(move_pct) & (np.abs(move_pct) >= float(min_move_abs))\n",
    "        if not good.any():\n",
    "            continue\n",
    "\n",
    "        s = s.iloc[good]\n",
    "        e = e.iloc[good]\n",
    "\n",
    "        move_pct = move_pct[good]\n",
    "        stack_delta = stack_delta[good]\n",
    "\n",
    "        direction = np.where(move_pct > 0.0, \"up\", \"down\")\n",
    "\n",
    "        ev = pd.DataFrame({\n",
    "            ticker_col: s[ticker_col].to_numpy(),\n",
    "            \"session_date\": s[\"session_date\"].to_numpy(),\n",
    "            \"class\": cls,\n",
    "            \"start_dt\": s[time_col].to_numpy(),\n",
    "            \"end_dt\": e[time_col].to_numpy(),\n",
    "            \"x_stack\": s[stack_col].to_numpy(dtype=float),\n",
    "            \"x_bench\": s[bench_col].to_numpy(dtype=float),\n",
    "            \"x_dev\": s[devsig_col].to_numpy(dtype=float),\n",
    "            \"move_pct\": move_pct,\n",
    "            \"stack_delta\": stack_delta,\n",
    "            \"dir\": direction,\n",
    "        })\n",
    "\n",
    "        if not ev.empty:\n",
    "            events_all.append(ev)\n",
    "\n",
    "    if not events_all:\n",
    "        raise ValueError(\"No events built.\")\n",
    "\n",
    "    events = pd.concat(events_all, ignore_index=True)\n",
    "\n",
    "    # =========================\n",
    "    # ===== SUMMARY APPEND ====\n",
    "    # =========================\n",
    "\n",
    "    rows = []\n",
    "    for cls, _ in class_defs:\n",
    "        evc = events[events[\"class\"] == cls]\n",
    "        if evc.empty:\n",
    "            continue\n",
    "\n",
    "        g = evc.groupby(ticker_col)\n",
    "        tmp = g.agg(\n",
    "            total=(\"dir\", \"count\"),\n",
    "            up=(\"dir\", lambda s: int((s == \"up\").sum())),\n",
    "            down=(\"dir\", lambda s: int((s == \"down\").sum())),\n",
    "            mean_move=(\"move_pct\", \"mean\"),\n",
    "            median_move=(\"move_pct\", \"median\"),\n",
    "            mean_stack_delta=(\"stack_delta\", \"mean\"),\n",
    "            median_stack_delta=(\"stack_delta\", \"median\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        tmp[f\"open_{cls}_total\"] = tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_up_rate\"] = tmp[\"up\"] / tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_down_rate\"] = tmp[\"down\"] / tmp[\"total\"]\n",
    "        tmp[f\"open_{cls}_mean_move\"] = tmp[\"mean_move\"]\n",
    "        tmp[f\"open_{cls}_median_move\"] = tmp[\"median_move\"]\n",
    "        tmp[f\"open_{cls}_mean_stack_delta\"] = tmp[\"mean_stack_delta\"]\n",
    "        tmp[f\"open_{cls}_median_stack_delta\"] = tmp[\"median_stack_delta\"]\n",
    "\n",
    "        tmp = tmp[[ticker_col] + [c for c in tmp.columns if c != ticker_col]]\n",
    "        rows.append(tmp)\n",
    "\n",
    "    if rows:\n",
    "        summary = rows[0]\n",
    "        for r in rows[1:]:\n",
    "            summary = summary.merge(r, on=ticker_col, how=\"outer\")\n",
    "\n",
    "        summary.to_csv(summary_path, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    # =========================\n",
    "    # ===== JSONL WRITE =======\n",
    "    # =========================\n",
    "\n",
    "    with _open_text(onefile_path, \"wt\") as onefile_out, \\\n",
    "         _open_text(best_params_path, \"at\") as best_out:\n",
    "\n",
    "        for ticker, ev_t in events.groupby(ticker_col):\n",
    "\n",
    "            rec = {\"ticker\": ticker, \"classes\": {}}\n",
    "            best = {\"ticker\": ticker, \"ratings\": {}, \"best_ranges\": {}}\n",
    "\n",
    "            for cls, evc in ev_t.groupby(\"class\"):\n",
    "                mv = evc[\"move_pct\"].to_numpy(dtype=float)\n",
    "                sd = evc[\"stack_delta\"].to_numpy(dtype=float)\n",
    "                is_up = (evc[\"dir\"].to_numpy() == \"up\")\n",
    "\n",
    "                total_cls = int(evc.shape[0])\n",
    "                up_rate_cls = float(is_up.mean()) if total_cls else 0.0\n",
    "                down_rate_cls = 1.0 - up_rate_cls if total_cls else 0.0\n",
    "\n",
    "                best[\"ratings\"][cls] = {\n",
    "                    \"any\": {\"rate\": float(max(up_rate_cls, down_rate_cls)), \"total\": total_cls},\n",
    "                    \"up\": {\"rate\": up_rate_cls, \"total\": total_cls},\n",
    "                    \"down\": {\"rate\": down_rate_cls, \"total\": total_cls},\n",
    "                }\n",
    "\n",
    "                rec[\"classes\"][cls] = {\n",
    "                    \"stats\": {\n",
    "                        \"total\": total_cls,\n",
    "                        \"up_rate\": up_rate_cls,\n",
    "                        \"down_rate\": down_rate_cls,\n",
    "                        \"mean_move\": float(np.mean(mv)) if mv.size else None,\n",
    "                        \"median_move\": float(np.median(mv)) if mv.size else None,\n",
    "                        \"mean_stack_delta\": float(np.mean(sd)) if sd.size else None,\n",
    "                        \"median_stack_delta\": float(np.median(sd)) if sd.size else None,\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            onefile_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            best_out.write(json.dumps(best, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"GOTOVO:\")\n",
    "    print(\" \", summary_path)\n",
    "    print(\" \", onefile_path)\n",
    "    print(\" \", best_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d69fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTOVO:\n",
      "  ARBITRAGE/open_analysis_fast\\summary.csv\n",
      "  ARBITRAGE/open_analysis_fast\\onefile.jsonl\n",
      "  ARBITRAGE/open_analysis_fast\\best_params.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def _resolve_orion_paths(strategy_code: str):\n",
    "    final_env = os.environ.get(\"FINAL_PARQUET_PATH\")\n",
    "    sig_env   = os.environ.get(\"SIGNALS_DIR\")\n",
    "    orion_env = os.environ.get(\"ORION_HOME\")\n",
    "\n",
    "    final_path = Path(final_env).expanduser().resolve() if final_env else None\n",
    "    signals_base = Path(sig_env).expanduser().resolve() if sig_env else None\n",
    "\n",
    "    if (final_path is None or signals_base is None) and orion_env:\n",
    "        orion_home = Path(orion_env).expanduser().resolve()\n",
    "        if final_path is None:\n",
    "            final_path = (orion_home / \"CRACEN\" / \"final.parquet\").resolve()\n",
    "        if signals_base is None:\n",
    "            signals_base = (orion_home / \"signals\").resolve()\n",
    "\n",
    "    if final_path is None or signals_base is None:\n",
    "        here = Path.cwd().resolve()\n",
    "        orion_home = None\n",
    "        for parent in [here] + list(here.parents):\n",
    "            if parent.name.lower() == \"orion\":\n",
    "                orion_home = parent\n",
    "                break\n",
    "            cand = parent / \"OriON\"\n",
    "            if cand.exists() and cand.is_dir():\n",
    "                orion_home = cand.resolve()\n",
    "                break\n",
    "\n",
    "        if orion_home is None:\n",
    "            raise RuntimeError(\"Cannot locate OriON. Set ORION_HOME (recommended).\")\n",
    "\n",
    "        if final_path is None:\n",
    "            final_path = (orion_home / \"CRACEN\" / \"final.parquet\").resolve()\n",
    "        if signals_base is None:\n",
    "            signals_base = (orion_home / \"signals\").resolve()\n",
    "\n",
    "    out_dir = (signals_base / strategy_code.lower()).resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not final_path.exists():\n",
    "        raise FileNotFoundError(f\"FINAL parquet not found: {final_path}\")\n",
    "\n",
    "    return final_path, out_dir\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MAIN (NO SORTING)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "FINAL_PATH, OUT_DIR = _resolve_orion_paths(\"open\")\n",
    "\n",
    "print(\"Using FINAL parquet (no pre-sorting):\", FINAL_PATH)\n",
    "\n",
    "analyze_open_strategy_fast(\n",
    "    input_parquet=str(FINAL_PATH),\n",
    "    output_dir=str(OUT_DIR),\n",
    "\n",
    "    n_bins_1d=10,\n",
    "    n_bins_2d=10,\n",
    "\n",
    "    min_move_abs=0.3,\n",
    "    minRateTop=0.6,\n",
    "    minTotalTop=4,\n",
    "\n",
    "    peak_time_bin_minutes=1,\n",
    ")\n",
    "\n",
    "print(\"OPEN analysis completed (no sorting).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
