{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ae8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_date = \"2026-01-01\"  # papermill replacement\n",
    "import os\n",
    "output_dir = os.environ.get(\"ORION_SIGNALS_DIR\", \"../signals\")\n",
    "config_path = os.environ.get(\"DATUM_API_CONFIG_PATH\", \"../ops/datum_api_config.json\")\n",
    "dry_run = False\n",
    "\n",
    "# ensure output exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0381d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic modules\n",
    "import pandas as pd\n",
    "from datum_api_client import DatumApi\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pip install xlrd\n",
    "# pip install openpyxl\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af3ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "def devsig_stream_stats_v12_exporter(\n",
    "    input_path: str,\n",
    "    *,\n",
    "    # === outputs ===\n",
    "    output_onefile_jsonl: str = \"ARBITRAGE/onefile.jsonl\",\n",
    "    output_summary_csv: str = \"ARBITRAGE/summary.csv\",\n",
    "    output_best_params_jsonl: str = \"ARBITRAGE/best_params.jsonl\",\n",
    "    # === include heavy parts ===\n",
    "    include_events_pre: bool = False,\n",
    "    include_events_intra: bool = False,\n",
    "    include_events_post: bool = False,\n",
    "    max_events_per_ticker: int = 500,\n",
    "    # === thresholds ===\n",
    "    dev_thr: float = 0.30,      # trigger (abs(dev_sig) >= dev_thr)\n",
    "    norm_thr: float = 0.10,     # HARD normalization threshold (abs(dev_sig) <= norm_thr)\n",
    "    soft_ratio: float = 3.0,    # SOFT: abs(dev_sig) <= peak_abs / soft_ratio\n",
    "    # === best params selection rules (kept; we ADD simpler ANY windows) ===\n",
    "    best_rules: \"dict|None\" = None,\n",
    "    # === reading ===\n",
    "    assume_sorted: bool = True,\n",
    "    parquet_use_pyarrow: bool = True,\n",
    "    parquet_iter_batches: bool = True,   # ✅ optional speedup (Step 4)\n",
    "    parquet_batch_size: int = 1_000_000, # ✅ batch size for iter_batches\n",
    "    csv_chunksize: int = 500_000,\n",
    "    log_every_n_chunks: int = 5,\n",
    "    # === bins ===\n",
    "    sigma_bin_min: float = 0.2,\n",
    "    sigma_bin_max: float = 2.7,\n",
    "    sigma_bin_step: float = 0.1,\n",
    "    bench_bin_min: float = -3.0,\n",
    "    bench_bin_max: float = 3.0,\n",
    "    bench_bin_step: float = 0.1,\n",
    "    # === time bands ===\n",
    "    start_band_minutes: int = 30,\n",
    "    norm_band_minutes: int = 30,\n",
    "    # === numeric fields stored in data ===\n",
    "    BENCH_NUM_FIELD: str = \"Bench%\",\n",
    "    STOCK_NUM_FIELD: str = \"Stack%\",\n",
    "    # === global filter for ALL outputs ===\n",
    "    min_events_per_ticker: int = 10,\n",
    "    # === open series ===\n",
    "    open_series_downsample_seconds: int = 60,  # 60s => 1 point / minute\n",
    "):\n",
    "    \"\"\"\n",
    "    v12 exporter UPDATED with BLUE + POST and strict \"parallel class checks\" semantics:\n",
    "\n",
    "    ✅ Classes:\n",
    "      - PRE classes: BLUE, ARK, PRINT, OPEN (all evaluated in parallel for the same PRE event)\n",
    "      - GLOBAL = priority selector over {BLUE, ARK, PRINT, OPEN} (POST NOT included)\n",
    "      - INTRA class (10:00–12:00)\n",
    "      - POST class (16:01–19:59) (separate event stream, not in GLOBAL)\n",
    "\n",
    "    ✅ GLOBAL priority:\n",
    "      BLUE_HARD > ARK_HARD > PRINT_HARD > BLUE_SOFT > ARK_SOFT > PRINT_SOFT > OPEN_HARD > OPEN_SOFT > NONE\n",
    "\n",
    "    ✅ BEST_PARAMS:\n",
    "      - best_windows_any stitched for ALL classes:\n",
    "        blue/ark/print/open/global/intra/post, each per sign (pos/neg)\n",
    "      - uses ANY = hard+soft normalization ratio, thresholds total>=4, rate>=0.6\n",
    "\n",
    "    ✅ IMPORTANT SEMANTICS:\n",
    "      - BLUE/ARK/PRINT/OPEN do NOT mute each other. They all get their own hard/soft/none outcome.\n",
    "      - PRE event is finalized after OPEN window (same as before).\n",
    "      - BLUE has its OWN peak (frozen until 03:59) and soft is evaluated vs BLUE peak.\n",
    "      - ARK/PRINT/OPEN use PRE peak frozen until 09:29.\n",
    "\n",
    "    ✅ PERFORMANCE PATCHES (no semantic changes):\n",
    "      - gzip outputs: .jsonl -> .jsonl.gz (if path endswith .gz)\n",
    "      - vectorized dt/dev parsing + ignored window filtering before loop\n",
    "      - avoid parse_dt()/hhmm()/is_ignored_time() per row\n",
    "      - reduce gc.collect() frequency\n",
    "      - optional pyarrow iter_batches() for parquet\n",
    "    \"\"\"\n",
    "    import os, gc, json, time, math, gzip\n",
    "    from collections import deque, defaultdict, Counter\n",
    "    from datetime import datetime, timedelta\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # ---------------- defaults for best rules (kept) ----------------\n",
    "    if best_rules is None:\n",
    "        best_rules = {\n",
    "            \"sigma_any\":  {\"min_rate\": 0.60, \"min_total\": 20, \"top_n\": 3},\n",
    "            \"sigma_hard\": {\"min_rate\": 0.55, \"min_total\": 20, \"top_n\": 3},\n",
    "            \"sigma_soft\": {\"min_rate\": 0.60, \"min_total\": 15, \"top_n\": 3},\n",
    "\n",
    "            \"bench_any\":  {\"min_rate\": 0.58, \"min_total\": 25, \"top_n\": 3},\n",
    "            \"bench_hard\": {\"min_rate\": 0.52, \"min_total\": 25, \"top_n\": 3},\n",
    "            \"bench_soft\": {\"min_rate\": 0.60, \"min_total\": 20, \"top_n\": 3},\n",
    "\n",
    "            \"soft_peak_rate\": {\"min_rate\": 0.55, \"min_total\": 15, \"top_n\": 3},\n",
    "            \"soft_low_rate\":  {\"min_rate\": 0.55, \"min_total\": 15, \"top_n\": 3},\n",
    "\n",
    "            \"start_band_any\": {\"min_rate\": 0.60, \"min_total\": 20, \"top_n\": 3},\n",
    "        }\n",
    "\n",
    "    # ---------------- gzip-aware open ----------------\n",
    "    def _open_text(path: str, mode: str = \"wt\"):\n",
    "        # mode expected: \"wt\" or \"at\"\n",
    "        if str(path).lower().endswith(\".gz\"):\n",
    "            return gzip.open(\n",
    "                path,\n",
    "                mode,\n",
    "                encoding=\"utf-8\",\n",
    "                newline=\"\\n\",\n",
    "                compresslevel=6,  # баланс швидкість/розмір\n",
    "            )\n",
    "        return open(path, mode.replace(\"t\", \"\"), encoding=\"utf-8\", newline=\"\\n\")\n",
    "\n",
    "    Path(output_onefile_jsonl).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(output_summary_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(output_best_params_jsonl).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    onefile_f = _open_text(output_onefile_jsonl, \"wt\")\n",
    "\n",
    "    summary_cols = [\n",
    "        \"ticker\", \"bench\", \"events_total\",\n",
    "        \"events_pre_total\", \"events_intra_total\", \"events_post_total\",\n",
    "\n",
    "        \"blue_any_rate\", \"blue_hard_rate\", \"blue_soft_rate\",\n",
    "        \"ark_any_rate\", \"ark_hard_rate\", \"ark_soft_rate\",\n",
    "        \"print_any_rate\", \"print_hard_rate\", \"print_soft_rate\",\n",
    "        \"open_any_rate\", \"open_hard_rate\", \"open_soft_rate\",\n",
    "        \"global_any_rate\", \"global_hard_rate\", \"global_soft_rate\",\n",
    "        \"intra_any_rate\", \"intra_hard_rate\", \"intra_soft_rate\",\n",
    "        \"post_any_rate\", \"post_hard_rate\", \"post_soft_rate\",\n",
    "\n",
    "        \"corr\", \"beta\", \"sigma\",\n",
    "    ]\n",
    "    pd.DataFrame(columns=summary_cols).to_csv(output_summary_csv, index=False, mode=\"w\")\n",
    "\n",
    "    best_params_f = _open_text(output_best_params_jsonl, \"wt\")\n",
    "    best_params_f.write(json.dumps({\n",
    "        \"meta\": {\"version\": \"v12+blue+post\", \"generated_at\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "    }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # ---------------- helpers ----------------\n",
    "    def _json_safe(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, (np.floating, float)):\n",
    "            if np.isnan(x) or np.isinf(x):\n",
    "                return None\n",
    "            return float(x)\n",
    "        if isinstance(x, (np.integer, int)):\n",
    "            return int(x)\n",
    "        if isinstance(x, (np.bool_, bool)):\n",
    "            return bool(x)\n",
    "        if isinstance(x, (pd.Timestamp,)):\n",
    "            return x.isoformat()\n",
    "        if isinstance(x, (datetime,)):\n",
    "            return x.isoformat()\n",
    "        return x\n",
    "\n",
    "    def is_finite_num(x) -> bool:\n",
    "        try:\n",
    "            return np.isfinite(float(x))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def as_float_or_nan(x) -> float:\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def hhmm(dt_obj):\n",
    "        return (dt_obj.hour, dt_obj.minute) if isinstance(dt_obj, datetime) else None\n",
    "\n",
    "    def in_range(t, a, b):\n",
    "        return (t is not None) and (a <= t <= b)\n",
    "\n",
    "    def _dt_iso(x):\n",
    "        return x.isoformat() if isinstance(x, datetime) else None\n",
    "\n",
    "    def floor_to_band(dt_obj: datetime, minutes: int) -> str:\n",
    "        if not isinstance(dt_obj, datetime):\n",
    "            return None\n",
    "        m = (dt_obj.minute // minutes) * minutes\n",
    "        start = dt_obj.replace(minute=m, second=0, microsecond=0)\n",
    "        end = start + timedelta(minutes=minutes)\n",
    "        return f\"{start.hour:02d}:{start.minute:02d}-{end.hour:02d}:{end.minute:02d}\"\n",
    "\n",
    "    # ---------------- time windows ----------------\n",
    "    BLUE_FROM = (0, 1)\n",
    "    BLUE_TO   = (3, 59)\n",
    "\n",
    "    ARK_FROM = (0, 5)\n",
    "    ARK_TO   = (9, 29)\n",
    "\n",
    "    PRINT_FROM = (9, 30)\n",
    "    PRINT_TO   = (9, 35)\n",
    "\n",
    "    OPEN_FROM  = (9, 31)\n",
    "    OPEN_TO    = (9, 40)\n",
    "\n",
    "    INTRA_FROM = (10, 0)\n",
    "    INTRA_TO   = (12, 0)\n",
    "\n",
    "    POST_FROM  = (16, 1)\n",
    "    POST_TO    = (19, 59)\n",
    "\n",
    "    # ✅ ignored windows (vectorized filtering uses these exact bounds, inclusive)\n",
    "    IGNORE_WINDOWS = [((3, 58), (4, 5)), ((7, 58), (8, 5))]\n",
    "\n",
    "    def is_ignored_time(t):\n",
    "        return any(in_range(t, a, b) for a, b in IGNORE_WINDOWS)\n",
    "\n",
    "    # ---------------- binning ----------------\n",
    "    def _clamp(v, lo, hi):\n",
    "        return max(lo, min(hi, v))\n",
    "\n",
    "    def sigma_bin(abs_sigma):\n",
    "        if not is_finite_num(abs_sigma):\n",
    "            return None\n",
    "        v = float(abs_sigma)\n",
    "        v = _clamp(v, sigma_bin_min, sigma_bin_max)\n",
    "        b = round(np.floor(v / sigma_bin_step) * sigma_bin_step, 1)\n",
    "        return f\"{b:.1f}\"\n",
    "\n",
    "    def bench_bin(val):\n",
    "        if not is_finite_num(val):\n",
    "            return None\n",
    "        v = float(val)\n",
    "        v = _clamp(v, bench_bin_min, bench_bin_max)\n",
    "        b = round(np.floor(v / bench_bin_step) * bench_bin_step, 1)\n",
    "        return f\"{b:.1f}\"\n",
    "\n",
    "    def _score(rate: float, total: int) -> float:\n",
    "        return float(rate) * math.log1p(int(total))\n",
    "\n",
    "    # ---- “simple ANY windows” selection (rate>=0.6 & total>=4) ----\n",
    "    def stitch_numeric_bin_intervals_from_any(\n",
    "        bin_counts: dict, *, step: float, min_total: int = 4, min_rate: float = 0.6\n",
    "    ):\n",
    "        eligible = []\n",
    "        for b_str, st in (bin_counts or {}).items():\n",
    "            try:\n",
    "                b = float(b_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "            total = int(st.get(\"total\", 0))\n",
    "            if total < min_total:\n",
    "                continue\n",
    "            any_ = int(st.get(\"hard\", 0)) + int(st.get(\"soft\", 0))\n",
    "            rate = any_ / total if total else 0.0\n",
    "            if rate >= min_rate:\n",
    "                eligible.append(b)\n",
    "\n",
    "        eligible.sort()\n",
    "        if not eligible:\n",
    "            return []\n",
    "\n",
    "        intervals = []\n",
    "        lo = hi = eligible[0]\n",
    "        for b in eligible[1:]:\n",
    "            if abs(b - (hi + step)) <= 1e-9:\n",
    "                hi = b\n",
    "            else:\n",
    "                intervals.append((lo, hi))\n",
    "                lo = hi = b\n",
    "        intervals.append((lo, hi))\n",
    "\n",
    "        out = []\n",
    "        for lo, hi in intervals:\n",
    "            tot = hard = soft = none = 0\n",
    "            k = lo\n",
    "            while k <= hi + 1e-9:\n",
    "                ks = f\"{k:.1f}\"\n",
    "                st = (bin_counts or {}).get(ks)\n",
    "                if st:\n",
    "                    tot += int(st.get(\"total\", 0))\n",
    "                    hard += int(st.get(\"hard\", 0))\n",
    "                    soft += int(st.get(\"soft\", 0))\n",
    "                    none += int(st.get(\"none\", 0))\n",
    "                k = round(k + step, 10)\n",
    "            if tot <= 0:\n",
    "                continue\n",
    "            rate = (hard + soft) / tot\n",
    "            if tot >= min_total and rate >= min_rate:\n",
    "                out.append({\n",
    "                    \"lo\": round(lo, 2),\n",
    "                    \"hi\": round(hi, 2),\n",
    "                    \"total\": int(tot),\n",
    "                    \"hard\": int(hard),\n",
    "                    \"soft\": int(soft),\n",
    "                    \"none\": int(none),\n",
    "                    \"rate\": float(rate),\n",
    "                    \"score\": _score(rate, tot),\n",
    "                })\n",
    "\n",
    "        out.sort(key=lambda x: (x[\"score\"], x[\"total\"]), reverse=True)\n",
    "        return out\n",
    "\n",
    "    def stitch_timeband_intervals_from_any(\n",
    "        band_counts: dict, *, min_total: int = 4, min_rate: float = 0.6\n",
    "    ):\n",
    "        def band_key_to_minutes(k: str):\n",
    "            try:\n",
    "                a, _b = k.split(\"-\")\n",
    "                h1, m1 = map(int, a.split(\":\"))\n",
    "                return h1 * 60 + m1\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        items = []\n",
    "        for k, st in (band_counts or {}).items():\n",
    "            tot = int(st.get(\"total\", 0))\n",
    "            if tot < min_total:\n",
    "                continue\n",
    "            any_ = int(st.get(\"hard\", 0)) + int(st.get(\"soft\", 0))\n",
    "            rate = any_ / tot if tot else 0.0\n",
    "            if rate >= min_rate:\n",
    "                km = band_key_to_minutes(k)\n",
    "                if km is not None:\n",
    "                    items.append((km, k))\n",
    "        items.sort()\n",
    "        if not items:\n",
    "            return []\n",
    "\n",
    "        stitched_groups = []\n",
    "        cur = [items[0][1]]\n",
    "        for _, k in items[1:]:\n",
    "            prev = cur[-1]\n",
    "            try:\n",
    "                prev_end = prev.split(\"-\")[1]\n",
    "                k_start = k.split(\"-\")[0]\n",
    "                if prev_end == k_start:\n",
    "                    cur.append(k)\n",
    "                else:\n",
    "                    stitched_groups.append(cur)\n",
    "                    cur = [k]\n",
    "            except Exception:\n",
    "                stitched_groups.append(cur)\n",
    "                cur = [k]\n",
    "        stitched_groups.append(cur)\n",
    "\n",
    "        stitched = []\n",
    "        for bands in stitched_groups:\n",
    "            tot = hard = soft = none = 0\n",
    "            for b in bands:\n",
    "                st = (band_counts or {}).get(b, {})\n",
    "                tot += int(st.get(\"total\", 0))\n",
    "                hard += int(st.get(\"hard\", 0))\n",
    "                soft += int(st.get(\"soft\", 0))\n",
    "                none += int(st.get(\"none\", 0))\n",
    "            if tot <= 0:\n",
    "                continue\n",
    "            rate = (hard + soft) / tot\n",
    "            if tot >= min_total and rate >= min_rate:\n",
    "                stitched.append({\n",
    "                    \"from\": bands[0],\n",
    "                    \"to\": bands[-1],\n",
    "                    \"bands\": bands,\n",
    "                    \"total\": int(tot),\n",
    "                    \"hard\": int(hard),\n",
    "                    \"soft\": int(soft),\n",
    "                    \"none\": int(none),\n",
    "                    \"rate\": float(rate),\n",
    "                    \"score\": _score(rate, tot),\n",
    "                })\n",
    "        stitched.sort(key=lambda x: (x[\"score\"], x[\"total\"]), reverse=True)\n",
    "        return stitched\n",
    "\n",
    "    # ---------------- global label priority (GLOBAL only) ----------------\n",
    "    GLOBAL_PRIORITY = [\n",
    "        \"BLUE_HARD\",\n",
    "        \"ARK_HARD\",\n",
    "        \"PRINT_HARD\",\n",
    "        \"BLUE_SOFT\",\n",
    "        \"ARK_SOFT\",\n",
    "        \"PRINT_SOFT\",\n",
    "        \"OPEN_HARD\",\n",
    "        \"OPEN_SOFT\",\n",
    "        \"NONE\",\n",
    "    ]\n",
    "\n",
    "    def compute_global_label(blue_status, ark_status, print_status, open_status):\n",
    "        if blue_status == \"hard\":\n",
    "            return \"BLUE_HARD\"\n",
    "        if ark_status == \"hard\":\n",
    "            return \"ARK_HARD\"\n",
    "        if print_status == \"hard\":\n",
    "            return \"PRINT_HARD\"\n",
    "        if blue_status == \"soft\":\n",
    "            return \"BLUE_SOFT\"\n",
    "        if ark_status == \"soft\":\n",
    "            return \"ARK_SOFT\"\n",
    "        if print_status == \"soft\":\n",
    "            return \"PRINT_SOFT\"\n",
    "        if open_status == \"hard\":\n",
    "            return \"OPEN_HARD\"\n",
    "        if open_status == \"soft\":\n",
    "            return \"OPEN_SOFT\"\n",
    "        return \"NONE\"\n",
    "\n",
    "    # ---------------- per-ticker state ----------------\n",
    "    cur_ticker = None\n",
    "    cur_day = None\n",
    "\n",
    "    bench_name_seen = None\n",
    "    static_triplet_set = False\n",
    "    corr_static = beta_static = sigma_static = None\n",
    "\n",
    "    # optional heavy buffers\n",
    "    pre_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "    intra_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "    post_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "\n",
    "    # counts per class\n",
    "    counts_pre = {\n",
    "        \"blue\": Counter(),\n",
    "        \"ark\": Counter(),\n",
    "        \"print\": Counter(),\n",
    "        \"open\": Counter(),\n",
    "        \"global\": Counter(),\n",
    "    }\n",
    "    global_labels_counter = Counter()\n",
    "    counts_intra = {\"intra\": Counter()}\n",
    "    counts_post = {\"post\": Counter()}\n",
    "\n",
    "    # sigma bins: class -> sign -> bin -> Counter(total/hard/soft/none)\n",
    "    def make_sigma_bins_map(classes):\n",
    "        m = {}\n",
    "        for c in classes:\n",
    "            m[c] = {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())}\n",
    "        return m\n",
    "\n",
    "    sigma_bins_pre = make_sigma_bins_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "    sigma_bins_intra = make_sigma_bins_map([\"intra\"])\n",
    "    sigma_bins_post = make_sigma_bins_map([\"post\"])\n",
    "\n",
    "    # bench bins\n",
    "    def make_bench_bins_map(classes):\n",
    "        out = {}\n",
    "        for c in classes:\n",
    "            out[c] = {\n",
    "                \"start\": {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())},\n",
    "                \"peak\":  {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())},\n",
    "                \"norm\":  {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())},\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    bench_bins_pre = make_bench_bins_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "    bench_bins_intra = make_bench_bins_map([\"intra\"])\n",
    "    bench_bins_post = make_bench_bins_map([\"post\"])\n",
    "\n",
    "    # time bands (OLD ones kept) — PRE/INTRA/POST each\n",
    "    start_bands_pre_total = Counter()\n",
    "    start_bands_pre_any   = Counter()\n",
    "    start_bands_pre_hard  = Counter()\n",
    "    start_bands_pre_soft  = Counter()\n",
    "    norm_bands_pre_any   = Counter()\n",
    "    norm_bands_pre_hard  = Counter()\n",
    "    norm_bands_pre_soft  = Counter()\n",
    "\n",
    "    start_bands_intra_total = Counter()\n",
    "    start_bands_intra_any   = Counter()\n",
    "    start_bands_intra_hard  = Counter()\n",
    "    start_bands_intra_soft  = Counter()\n",
    "    norm_bands_intra_any   = Counter()\n",
    "    norm_bands_intra_hard  = Counter()\n",
    "    norm_bands_intra_soft  = Counter()\n",
    "\n",
    "    start_bands_post_total = Counter()\n",
    "    start_bands_post_any   = Counter()\n",
    "    start_bands_post_hard  = Counter()\n",
    "    start_bands_post_soft  = Counter()\n",
    "    norm_bands_post_any   = Counter()\n",
    "    norm_bands_post_hard  = Counter()\n",
    "    norm_bands_post_soft  = Counter()\n",
    "\n",
    "    # NEW: time bands per class+sign with total/hard/soft/none\n",
    "    def make_timeband_map(classes):\n",
    "        out = {}\n",
    "        for c in classes:\n",
    "            out[c] = {\n",
    "                \"start\": {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())},\n",
    "                \"norm\":  {\"pos\": defaultdict(lambda: Counter()), \"neg\": defaultdict(lambda: Counter())},\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    timebands_pre_by_class_sign = make_timeband_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "    timebands_intra_by_class_sign = make_timeband_map([\"intra\"])\n",
    "    timebands_post_by_class_sign = make_timeband_map([\"post\"])\n",
    "\n",
    "    # ✅ last3 examples per class AND per sign\n",
    "    def make_last3_map(classes):\n",
    "        return {c: {\"pos\": deque(maxlen=3), \"neg\": deque(maxlen=3)} for c in classes}\n",
    "\n",
    "    last3_examples = make_last3_map([\"blue\", \"ark\", \"print\", \"open\", \"global\", \"intra\", \"post\"])\n",
    "\n",
    "    # RECENT by DAYS (kept)\n",
    "    recent_days = deque(maxlen=10)  # day strings YYYY-MM-DD\n",
    "    recent_by_day = {}  # day -> {\"print\":..., \"peak\":...}\n",
    "\n",
    "    last5_print_days_pos = deque(maxlen=5)\n",
    "    last5_print_days_neg = deque(maxlen=5)\n",
    "\n",
    "    last5_peak_days_pos = deque(maxlen=5)\n",
    "    last5_peak_days_neg = deque(maxlen=5)\n",
    "\n",
    "    # HARD delays (kept + blue/post)\n",
    "    hard_delay_sum = Counter()\n",
    "    hard_delay_cnt = Counter()\n",
    "\n",
    "    # mean peak_abs for globally normalized events (kept) — now picks BLUE peak if global is BLUE\n",
    "    global_norm_peak_sum = {\"pos\": 0.0, \"neg\": 0.0}\n",
    "    global_norm_peak_cnt = {\"pos\": 0, \"neg\": 0}\n",
    "\n",
    "    # OPEN dev_sig series for last10 days (downsample by seconds)\n",
    "    open_series_by_day = {}\n",
    "\n",
    "    # ---------------- reset ticker ----------------\n",
    "    def reset_ticker_state():\n",
    "        nonlocal bench_name_seen, static_triplet_set, corr_static, beta_static, sigma_static\n",
    "        nonlocal pre_events_buf, intra_events_buf, post_events_buf\n",
    "        nonlocal counts_pre, global_labels_counter, counts_intra, counts_post\n",
    "        nonlocal sigma_bins_pre, sigma_bins_intra, sigma_bins_post\n",
    "        nonlocal bench_bins_pre, bench_bins_intra, bench_bins_post\n",
    "        nonlocal start_bands_pre_total, start_bands_pre_any, start_bands_pre_hard, start_bands_pre_soft\n",
    "        nonlocal norm_bands_pre_any, norm_bands_pre_hard, norm_bands_pre_soft\n",
    "        nonlocal start_bands_intra_total, start_bands_intra_any, start_bands_intra_hard, start_bands_intra_soft\n",
    "        nonlocal norm_bands_intra_any, norm_bands_intra_hard, norm_bands_intra_soft\n",
    "        nonlocal start_bands_post_total, start_bands_post_any, start_bands_post_hard, start_bands_post_soft\n",
    "        nonlocal norm_bands_post_any, norm_bands_post_hard, norm_bands_post_soft\n",
    "        nonlocal timebands_pre_by_class_sign, timebands_intra_by_class_sign, timebands_post_by_class_sign\n",
    "        nonlocal last3_examples\n",
    "        nonlocal recent_days, recent_by_day, last5_print_days_pos, last5_print_days_neg\n",
    "        nonlocal last5_peak_days_pos, last5_peak_days_neg\n",
    "        nonlocal hard_delay_sum, hard_delay_cnt\n",
    "        nonlocal global_norm_peak_sum, global_norm_peak_cnt\n",
    "        nonlocal open_series_by_day\n",
    "\n",
    "        bench_name_seen = None\n",
    "        static_triplet_set = False\n",
    "        corr_static = beta_static = sigma_static = None\n",
    "\n",
    "        pre_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "        intra_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "        post_events_buf = deque(maxlen=max_events_per_ticker)\n",
    "\n",
    "        counts_pre = {\"blue\": Counter(), \"ark\": Counter(), \"print\": Counter(), \"open\": Counter(), \"global\": Counter()}\n",
    "        global_labels_counter = Counter()\n",
    "        counts_intra = {\"intra\": Counter()}\n",
    "        counts_post = {\"post\": Counter()}\n",
    "\n",
    "        sigma_bins_pre = make_sigma_bins_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "        sigma_bins_intra = make_sigma_bins_map([\"intra\"])\n",
    "        sigma_bins_post = make_sigma_bins_map([\"post\"])\n",
    "\n",
    "        bench_bins_pre = make_bench_bins_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "        bench_bins_intra = make_bench_bins_map([\"intra\"])\n",
    "        bench_bins_post = make_bench_bins_map([\"post\"])\n",
    "\n",
    "        start_bands_pre_total = Counter()\n",
    "        start_bands_pre_any   = Counter()\n",
    "        start_bands_pre_hard  = Counter()\n",
    "        start_bands_pre_soft  = Counter()\n",
    "        norm_bands_pre_any   = Counter()\n",
    "        norm_bands_pre_hard  = Counter()\n",
    "        norm_bands_pre_soft  = Counter()\n",
    "\n",
    "        start_bands_intra_total = Counter()\n",
    "        start_bands_intra_any   = Counter()\n",
    "        start_bands_intra_hard  = Counter()\n",
    "        start_bands_intra_soft  = Counter()\n",
    "        norm_bands_intra_any   = Counter()\n",
    "        norm_bands_intra_hard  = Counter()\n",
    "        norm_bands_intra_soft  = Counter()\n",
    "\n",
    "        start_bands_post_total = Counter()\n",
    "        start_bands_post_any   = Counter()\n",
    "        start_bands_post_hard  = Counter()\n",
    "        start_bands_post_soft  = Counter()\n",
    "        norm_bands_post_any   = Counter()\n",
    "        norm_bands_post_hard  = Counter()\n",
    "        norm_bands_post_soft  = Counter()\n",
    "\n",
    "        timebands_pre_by_class_sign = make_timeband_map([\"blue\", \"ark\", \"print\", \"open\", \"global\"])\n",
    "        timebands_intra_by_class_sign = make_timeband_map([\"intra\"])\n",
    "        timebands_post_by_class_sign = make_timeband_map([\"post\"])\n",
    "\n",
    "        last3_examples = make_last3_map([\"blue\", \"ark\", \"print\", \"open\", \"global\", \"intra\", \"post\"])\n",
    "\n",
    "        recent_days = deque(maxlen=10)\n",
    "        recent_by_day = {}\n",
    "        last5_print_days_pos = deque(maxlen=5)\n",
    "        last5_print_days_neg = deque(maxlen=5)\n",
    "        last5_peak_days_pos = deque(maxlen=5)\n",
    "        last5_peak_days_neg = deque(maxlen=5)\n",
    "\n",
    "        hard_delay_sum = Counter()\n",
    "        hard_delay_cnt = Counter()\n",
    "\n",
    "        global_norm_peak_sum = {\"pos\": 0.0, \"neg\": 0.0}\n",
    "        global_norm_peak_cnt = {\"pos\": 0, \"neg\": 0}\n",
    "\n",
    "        open_series_by_day = {}\n",
    "\n",
    "    # ---------------- common utils ----------------\n",
    "\n",
    "    def parse_dt(x):\n",
    "        \"\"\"Уніфіковано привести вхід до datetime або None.\n",
    "        Підтримує: datetime, pd.Timestamp, ISO-рядки; повертає Python datetime (може бути tz-aware) або None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if x is None:\n",
    "                return None\n",
    "            if isinstance(x, datetime):\n",
    "                return x\n",
    "            # pandas поверне pd.Timestamp; встановлюємо utc=True щоб уникнути неоднозначностей\n",
    "            ts = pd.to_datetime(x, errors=\"coerce\", utc=True)\n",
    "            if pd.isna(ts):\n",
    "                return None\n",
    "            return ts.to_pydatetime()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def push_last3_example(class_key, sign_key, kind, start_dt, end_dt, start_dev, end_dev, peak_dev,\n",
    "                           start_stock, end_stock, start_bench, end_bench, start_time=None, end_time=None):\n",
    "        d = start_dt.date().isoformat() if isinstance(start_dt, datetime) else None\n",
    "        last3_examples[class_key][sign_key].appendleft({\n",
    "            \"date\": d,\n",
    "            \"dt\": _dt_iso(start_dt),\n",
    "            \"kind\": kind,  # \"hard\"/\"soft\"\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"start_dev\": _json_safe(start_dev),\n",
    "            \"peak_dev\": _json_safe(peak_dev),\n",
    "            \"end_dev\": _json_safe(end_dev),\n",
    "            \"stock_start\": _json_safe(start_stock),\n",
    "            \"stock_end\": _json_safe(end_stock),\n",
    "            \"bench_start\": _json_safe(start_bench),\n",
    "            \"bench_end\": _json_safe(end_bench),\n",
    "        })\n",
    "\n",
    "    def update_sigma_bins(map_ref, class_key, sign_key, abs_peak_sigma, outcome_kind):\n",
    "        b = sigma_bin(abs_peak_sigma)\n",
    "        if b is None:\n",
    "            return\n",
    "        st = map_ref[class_key][sign_key][b]\n",
    "        st[\"total\"] += 1\n",
    "        if outcome_kind not in (\"hard\", \"soft\", \"none\"):\n",
    "            outcome_kind = \"none\"\n",
    "        st[outcome_kind] += 1\n",
    "\n",
    "    def update_bench_bins(map_ref, class_key, which, sign_key, bench_value, outcome_kind):\n",
    "        b = bench_bin(bench_value)\n",
    "        if b is None:\n",
    "            return\n",
    "        st = map_ref[class_key][which][sign_key][b]\n",
    "        st[\"total\"] += 1\n",
    "        if outcome_kind not in (\"hard\", \"soft\", \"none\"):\n",
    "            outcome_kind = \"none\"\n",
    "        st[outcome_kind] += 1\n",
    "\n",
    "    def update_timeband_by_class_sign(map_ref, class_key, which, sign_key, band_key, outcome_kind):\n",
    "        if not band_key:\n",
    "            return\n",
    "        st = map_ref[class_key][which][sign_key][band_key]\n",
    "        st[\"total\"] += 1\n",
    "        if outcome_kind not in (\"hard\", \"soft\", \"none\"):\n",
    "            outcome_kind = \"none\"\n",
    "        st[outcome_kind] += 1\n",
    "        _ = st[\"hard\"]; _ = st[\"soft\"]; _ = st[\"none\"]\n",
    "\n",
    "    def class_rates(counter: Counter):\n",
    "        total = int(sum(counter.values()))\n",
    "        hard = int(counter.get(\"hard\", 0))\n",
    "        soft = int(counter.get(\"soft\", 0))\n",
    "        none = int(counter.get(\"none\", 0))\n",
    "        any_ = hard + soft\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"hard\": hard,\n",
    "            \"soft\": soft,\n",
    "            \"none\": none,\n",
    "            \"rate_any\": (any_ / total) if total else None,\n",
    "            \"rate_hard\": (hard / total) if total else None,\n",
    "            \"rate_soft\": (soft / total) if total else None,\n",
    "            \"hard_share_in_norm\": (hard / (hard + soft)) if (hard + soft) else None,\n",
    "        }\n",
    "\n",
    "    def add_hard_delay(key: str, start_dt: datetime, hard_dt: datetime):\n",
    "        if isinstance(start_dt, datetime) and isinstance(hard_dt, datetime) and hard_dt >= start_dt:\n",
    "            hard_delay_sum[key] += (hard_dt - start_dt).total_seconds()\n",
    "            hard_delay_cnt[key] += 1\n",
    "\n",
    "    def avg_hard_delay(key: str):\n",
    "        c = int(hard_delay_cnt.get(key, 0))\n",
    "        if c <= 0:\n",
    "            return None\n",
    "        return float(hard_delay_sum.get(key, 0.0)) / c\n",
    "\n",
    "    # ---------------- PRE event state ----------------\n",
    "    pre_active = False\n",
    "    pre_id = 0\n",
    "\n",
    "    pre_start_dt = None\n",
    "    pre_start_dev = np.nan\n",
    "    pre_start_sign = 0\n",
    "    pre_start_stock = np.nan\n",
    "    pre_start_bench = np.nan\n",
    "\n",
    "    # PRE peak frozen until 09:29 (for ARK/PRINT/OPEN/GLOBAL)\n",
    "    pre_peak_abs = 0.0\n",
    "    pre_peak_signed = 0.0\n",
    "    pre_peak_dt = None\n",
    "    pre_peak_stock = np.nan\n",
    "    pre_peak_bench = np.nan\n",
    "\n",
    "    pre_post_peak_low_abs = np.inf\n",
    "\n",
    "    # BLUE peak frozen until 03:59 (for BLUE soft)\n",
    "    blue_peak_abs = 0.0\n",
    "    blue_peak_signed = 0.0\n",
    "    blue_peak_dt = None\n",
    "    blue_peak_stock = np.nan\n",
    "    blue_peak_bench = np.nan\n",
    "\n",
    "    blue_hard_dt = None\n",
    "    blue_hard_val = np.nan\n",
    "    blue_hard_stock = np.nan\n",
    "    blue_hard_bench = np.nan\n",
    "\n",
    "    blue_soft_found = False\n",
    "    blue_soft_dt = None\n",
    "    blue_soft_val = np.nan\n",
    "    blue_soft_stock = np.nan\n",
    "    blue_soft_bench = np.nan\n",
    "\n",
    "    ark_hard_dt = None\n",
    "    ark_hard_val = np.nan\n",
    "    ark_hard_stock = np.nan\n",
    "    ark_hard_bench = np.nan\n",
    "\n",
    "    ark_soft_found = False\n",
    "    ark_soft_dt = None\n",
    "    ark_soft_val = np.nan\n",
    "    ark_soft_stock = np.nan\n",
    "    ark_soft_bench = np.nan\n",
    "\n",
    "    print_first_dt = None\n",
    "    print_first_val = np.nan\n",
    "    print_first_stock = np.nan\n",
    "    print_first_bench = np.nan\n",
    "\n",
    "    open_hard_dt = None\n",
    "    open_hard_val = np.nan\n",
    "    open_hard_stock = np.nan\n",
    "    open_hard_bench = np.nan\n",
    "\n",
    "    open_soft_found = False\n",
    "    open_soft_dt = None\n",
    "    open_soft_val = np.nan\n",
    "    open_soft_stock = np.nan\n",
    "    open_soft_bench = np.nan\n",
    "\n",
    "    def reset_pre_event():\n",
    "        nonlocal pre_active, pre_start_dt, pre_start_dev, pre_start_sign, pre_start_stock, pre_start_bench\n",
    "        nonlocal pre_peak_abs, pre_peak_signed, pre_peak_dt, pre_peak_stock, pre_peak_bench\n",
    "        nonlocal pre_post_peak_low_abs\n",
    "        nonlocal blue_peak_abs, blue_peak_signed, blue_peak_dt, blue_peak_stock, blue_peak_bench\n",
    "        nonlocal blue_hard_dt, blue_hard_val, blue_hard_stock, blue_hard_bench\n",
    "        nonlocal blue_soft_found, blue_soft_dt, blue_soft_val, blue_soft_stock, blue_soft_bench\n",
    "        nonlocal ark_hard_dt, ark_hard_val, ark_hard_stock, ark_hard_bench\n",
    "        nonlocal ark_soft_found, ark_soft_dt, ark_soft_val, ark_soft_stock, ark_soft_bench\n",
    "        nonlocal print_first_dt, print_first_val, print_first_stock, print_first_bench\n",
    "        nonlocal open_hard_dt, open_hard_val, open_hard_stock, open_hard_bench\n",
    "        nonlocal open_soft_found, open_soft_dt, open_soft_val, open_soft_stock, open_soft_bench\n",
    "\n",
    "        pre_active = False\n",
    "        pre_start_dt = None\n",
    "        pre_start_dev = np.nan\n",
    "        pre_start_sign = 0\n",
    "        pre_start_stock = np.nan\n",
    "        pre_start_bench = np.nan\n",
    "\n",
    "        pre_peak_abs = 0.0\n",
    "        pre_peak_signed = 0.0\n",
    "        pre_peak_dt = None\n",
    "        pre_peak_stock = np.nan\n",
    "        pre_peak_bench = np.nan\n",
    "\n",
    "        pre_post_peak_low_abs = np.inf\n",
    "\n",
    "        blue_peak_abs = 0.0\n",
    "        blue_peak_signed = 0.0\n",
    "        blue_peak_dt = None\n",
    "        blue_peak_stock = np.nan\n",
    "        blue_peak_bench = np.nan\n",
    "\n",
    "        blue_hard_dt = None\n",
    "        blue_hard_val = np.nan\n",
    "        blue_hard_stock = np.nan\n",
    "        blue_hard_bench = np.nan\n",
    "\n",
    "        blue_soft_found = False\n",
    "        blue_soft_dt = None\n",
    "        blue_soft_val = np.nan\n",
    "        blue_soft_stock = np.nan\n",
    "        blue_soft_bench = np.nan\n",
    "\n",
    "        ark_hard_dt = None\n",
    "        ark_hard_val = np.nan\n",
    "        ark_hard_stock = np.nan\n",
    "        ark_hard_bench = np.nan\n",
    "\n",
    "        ark_soft_found = False\n",
    "        ark_soft_dt = None\n",
    "        ark_soft_val = np.nan\n",
    "        ark_soft_stock = np.nan\n",
    "        ark_soft_bench = np.nan\n",
    "\n",
    "        print_first_dt = None\n",
    "        print_first_val = np.nan\n",
    "        print_first_stock = np.nan\n",
    "        print_first_bench = np.nan\n",
    "\n",
    "        open_hard_dt = None\n",
    "        open_hard_val = np.nan\n",
    "        open_hard_stock = np.nan\n",
    "        open_hard_bench = np.nan\n",
    "\n",
    "        open_soft_found = False\n",
    "        open_soft_dt = None\n",
    "        open_soft_val = np.nan\n",
    "        open_soft_stock = np.nan\n",
    "        open_soft_bench = np.nan\n",
    "\n",
    "    def start_pre_event(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal pre_active, pre_start_dt, pre_start_dev, pre_start_sign, pre_start_stock, pre_start_bench\n",
    "        nonlocal pre_peak_abs, pre_peak_signed, pre_peak_dt, pre_peak_stock, pre_peak_bench\n",
    "        nonlocal blue_peak_abs, blue_peak_signed, blue_peak_dt, blue_peak_stock, blue_peak_bench\n",
    "\n",
    "        pre_active = True\n",
    "        pre_start_dt = dt_now\n",
    "        pre_start_dev = float(dev_now)\n",
    "        pre_start_sign = 1 if float(dev_now) >= 0 else -1\n",
    "        pre_start_stock = stock_pct\n",
    "        pre_start_bench = bench_pct\n",
    "\n",
    "        # PRE peak init (shared)\n",
    "        pre_peak_abs = abs(float(dev_now))\n",
    "        pre_peak_signed = float(dev_now)\n",
    "        pre_peak_dt = dt_now\n",
    "        pre_peak_stock = stock_pct\n",
    "        pre_peak_bench = bench_pct\n",
    "\n",
    "        # BLUE peak init (so BLUE soft works even if event starts in BLUE)\n",
    "        blue_peak_abs = abs(float(dev_now))\n",
    "        blue_peak_signed = float(dev_now)\n",
    "        blue_peak_dt = dt_now\n",
    "        blue_peak_stock = stock_pct\n",
    "        blue_peak_bench = bench_pct\n",
    "\n",
    "    def pre_sign_key():\n",
    "        return \"pos\" if pre_start_sign > 0 else \"neg\"\n",
    "\n",
    "    def classify_print_with_frozen_peak(first_val):\n",
    "        if not is_finite_num(first_val):\n",
    "            return \"none\"\n",
    "        a = abs(float(first_val))\n",
    "        if a <= norm_thr:\n",
    "            return \"hard\"\n",
    "        if pre_peak_abs > 0 and a <= (float(pre_peak_abs) / float(soft_ratio)):\n",
    "            return \"soft\"\n",
    "        return \"none\"\n",
    "\n",
    "    def classify_blue():\n",
    "        if blue_hard_dt is not None and is_finite_num(blue_hard_val):\n",
    "            return \"hard\"\n",
    "        if blue_soft_found and blue_soft_dt is not None and is_finite_num(blue_soft_val):\n",
    "            return \"soft\"\n",
    "        return \"none\"\n",
    "\n",
    "    def classify_ark():\n",
    "        if ark_hard_dt is not None and is_finite_num(ark_hard_val):\n",
    "            return \"hard\"\n",
    "        if ark_soft_found and ark_soft_dt is not None and is_finite_num(ark_soft_val):\n",
    "            return \"soft\"\n",
    "        return \"none\"\n",
    "\n",
    "    def classify_open():\n",
    "        if open_hard_dt is not None and is_finite_num(open_hard_val):\n",
    "            return \"hard\"\n",
    "        if open_soft_found and open_soft_dt is not None and is_finite_num(open_soft_val):\n",
    "            return \"soft\"\n",
    "        return \"none\"\n",
    "\n",
    "    def capture_open_series(dt_now: datetime, dev_now: float):\n",
    "        if not isinstance(dt_now, datetime):\n",
    "            return\n",
    "        t = (dt_now.hour, dt_now.minute)\n",
    "        if not in_range(t, OPEN_FROM, OPEN_TO):\n",
    "            return\n",
    "\n",
    "        day_str = dt_now.date().isoformat()\n",
    "        store = open_series_by_day.get(day_str)\n",
    "        if store is None:\n",
    "            store = {}\n",
    "            open_series_by_day[day_str] = store\n",
    "\n",
    "        sec = max(1, int(open_series_downsample_seconds))\n",
    "        bucket_epoch = int(dt_now.timestamp() // sec) * sec\n",
    "        # use pandas to create tz-aware Timestamp consistently\n",
    "        bucket_dt = pd.Timestamp(bucket_epoch, unit=\"s\", tz=dt_now.tzinfo).to_pydatetime()\n",
    "        store[bucket_dt.isoformat()] = float(dev_now)\n",
    "\n",
    "    def pre_process_tick(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal pre_peak_abs, pre_peak_signed, pre_peak_dt, pre_peak_stock, pre_peak_bench\n",
    "        nonlocal pre_post_peak_low_abs\n",
    "\n",
    "        nonlocal blue_peak_abs, blue_peak_signed, blue_peak_dt, blue_peak_stock, blue_peak_bench\n",
    "        nonlocal blue_hard_dt, blue_hard_val, blue_hard_stock, blue_hard_bench\n",
    "        nonlocal blue_soft_found, blue_soft_dt, blue_soft_val, blue_soft_stock, blue_soft_bench\n",
    "\n",
    "        nonlocal ark_hard_dt, ark_hard_val, ark_hard_stock, ark_hard_bench\n",
    "        nonlocal ark_soft_found, ark_soft_dt, ark_soft_val, ark_soft_stock, ark_soft_bench\n",
    "\n",
    "        nonlocal print_first_dt, print_first_val, print_first_stock, print_first_bench\n",
    "\n",
    "        nonlocal open_hard_dt, open_hard_val, open_hard_stock, open_hard_bench\n",
    "        nonlocal open_soft_found, open_soft_dt, open_soft_val, open_soft_stock, open_soft_bench\n",
    "\n",
    "        t = (dt_now.hour, dt_now.minute)\n",
    "        cur_abs = abs(float(dev_now))\n",
    "\n",
    "        # capture open series (kept)\n",
    "        capture_open_series(dt_now, float(dev_now))\n",
    "\n",
    "        # ---------- BLUE processing inside 00:01–03:59 (parallel) ----------\n",
    "        if in_range(t, BLUE_FROM, BLUE_TO):\n",
    "            # BLUE peak frozen until 03:59\n",
    "            if cur_abs > float(blue_peak_abs):\n",
    "                blue_peak_abs = float(cur_abs)\n",
    "                blue_peak_signed = float(dev_now)\n",
    "                blue_peak_dt = dt_now\n",
    "                blue_peak_stock = stock_pct\n",
    "                blue_peak_bench = bench_pct\n",
    "                if blue_hard_dt is None:\n",
    "                    blue_soft_found = False\n",
    "                    blue_soft_dt = None\n",
    "                    blue_soft_val = np.nan\n",
    "                    blue_soft_stock = np.nan\n",
    "                    blue_soft_bench = np.nan\n",
    "\n",
    "            # BLUE HARD\n",
    "            if blue_hard_dt is None and cur_abs <= norm_thr:\n",
    "                blue_hard_dt = dt_now\n",
    "                blue_hard_val = float(dev_now)\n",
    "                blue_hard_stock = stock_pct\n",
    "                blue_hard_bench = bench_pct\n",
    "\n",
    "            # BLUE SOFT only if hard not yet, after BLUE peak\n",
    "            if (blue_hard_dt is None) and isinstance(blue_peak_dt, datetime) and (dt_now >= blue_peak_dt) and blue_peak_abs > 0:\n",
    "                if cur_abs <= (float(blue_peak_abs) / float(soft_ratio)):\n",
    "                    if (not blue_soft_found) or (dt_now < blue_soft_dt):\n",
    "                        blue_soft_found = True\n",
    "                        blue_soft_dt = dt_now\n",
    "                        blue_soft_val = float(dev_now)\n",
    "                        blue_soft_stock = stock_pct\n",
    "                        blue_soft_bench = bench_pct\n",
    "\n",
    "        # ---------- PRE peak (for ARK/PRINT/OPEN/GLOBAL) frozen until 09:29 ----------\n",
    "        if t <= ARK_TO:\n",
    "            if cur_abs > float(pre_peak_abs):\n",
    "                pre_peak_abs = float(cur_abs)\n",
    "                pre_peak_signed = float(dev_now)\n",
    "                pre_peak_dt = dt_now\n",
    "                pre_peak_stock = stock_pct\n",
    "                pre_peak_bench = bench_pct\n",
    "                if ark_hard_dt is None:\n",
    "                    ark_soft_found = False\n",
    "                    ark_soft_dt = None\n",
    "                    ark_soft_val = np.nan\n",
    "                    ark_soft_stock = np.nan\n",
    "                    ark_soft_bench = np.nan\n",
    "\n",
    "        # post-peak low abs after peak (kept)\n",
    "        if isinstance(pre_peak_dt, datetime) and dt_now >= pre_peak_dt:\n",
    "            if cur_abs < pre_post_peak_low_abs:\n",
    "                pre_post_peak_low_abs = cur_abs\n",
    "\n",
    "        # ARK HARD in [start..09:29]\n",
    "        if (ark_hard_dt is None) and (t <= ARK_TO):\n",
    "            if cur_abs <= norm_thr:\n",
    "                ark_hard_dt = dt_now\n",
    "                ark_hard_val = float(dev_now)\n",
    "                ark_hard_stock = stock_pct\n",
    "                ark_hard_bench = bench_pct\n",
    "\n",
    "        # ARK SOFT only if hard failed, in [peak_dt..09:29]\n",
    "        if (ark_hard_dt is None) and (t <= ARK_TO) and isinstance(pre_peak_dt, datetime):\n",
    "            if dt_now >= pre_peak_dt and pre_peak_abs > 0:\n",
    "                if cur_abs <= (float(pre_peak_abs) / float(soft_ratio)):\n",
    "                    if (not ark_soft_found) or (dt_now < ark_soft_dt):\n",
    "                        ark_soft_found = True\n",
    "                        ark_soft_dt = dt_now\n",
    "                        ark_soft_val = float(dev_now)\n",
    "                        ark_soft_stock = stock_pct\n",
    "                        ark_soft_bench = bench_pct\n",
    "\n",
    "        # PRINT first tick only in [09:30..09:35]\n",
    "        if (print_first_dt is None) and in_range(t, PRINT_FROM, PRINT_TO):\n",
    "            print_first_dt = dt_now\n",
    "            print_first_val = float(dev_now)\n",
    "            print_first_stock = stock_pct\n",
    "            print_first_bench = bench_pct\n",
    "\n",
    "        # OPEN scan in [09:31..09:40]\n",
    "        if in_range(t, OPEN_FROM, OPEN_TO):\n",
    "            if open_hard_dt is None and cur_abs <= norm_thr:\n",
    "                open_hard_dt = dt_now\n",
    "                open_hard_val = float(dev_now)\n",
    "                open_hard_stock = stock_pct\n",
    "                open_hard_bench = bench_pct\n",
    "\n",
    "            if open_hard_dt is None and pre_peak_abs > 0:\n",
    "                gate_dt = pre_peak_dt if isinstance(pre_peak_dt, datetime) else dt_now\n",
    "                open_gate = dt_now.replace(hour=OPEN_FROM[0], minute=OPEN_FROM[1], second=0, microsecond=0)\n",
    "                if gate_dt < open_gate:\n",
    "                    gate_dt = open_gate\n",
    "                if dt_now >= gate_dt:\n",
    "                    if cur_abs <= (float(pre_peak_abs) / float(soft_ratio)):\n",
    "                        if (not open_soft_found) or (dt_now < open_soft_dt):\n",
    "                            open_soft_found = True\n",
    "                            open_soft_dt = dt_now\n",
    "                            open_soft_val = float(dev_now)\n",
    "                            open_soft_stock = stock_pct\n",
    "                            open_soft_bench = bench_pct\n",
    "\n",
    "    def _ensure_recent_day(day_str: str):\n",
    "        if day_str not in recent_by_day:\n",
    "            recent_by_day[day_str] = {\"print\": None, \"peak\": None}\n",
    "        if (len(recent_days) == 0) or (recent_days[0] != day_str):\n",
    "            if day_str in recent_days:\n",
    "                recent_days.remove(day_str)\n",
    "            recent_days.appendleft(day_str)\n",
    "\n",
    "    def _update_recent_daily_print(day_str: str, print_dt: datetime, print_dev: float, peak_abs: float, peak_signed: float, first_sign: int):\n",
    "        snap = recent_by_day.get(day_str)\n",
    "        if snap is None:\n",
    "            recent_by_day[day_str] = {\"print\": None, \"peak\": None}\n",
    "            snap = recent_by_day[day_str]\n",
    "        if snap[\"print\"] is None:\n",
    "            snap[\"print\"] = {\n",
    "                \"dt\": _dt_iso(print_dt),\n",
    "                \"dev\": _json_safe(print_dev),\n",
    "                \"pre_peak_abs\": _json_safe(peak_abs),\n",
    "                \"pre_peak_signed\": _json_safe(peak_signed),\n",
    "                \"first_sign\": int(first_sign),\n",
    "            }\n",
    "\n",
    "    def _update_recent_daily_peak(day_str: str, peak_dt: datetime, peak_abs: float, peak_signed: float, first_sign: int):\n",
    "        snap = recent_by_day.get(day_str)\n",
    "        if snap is None:\n",
    "            recent_by_day[day_str] = {\"print\": None, \"peak\": None}\n",
    "            snap = recent_by_day[day_str]\n",
    "        cur = snap[\"peak\"]\n",
    "        if cur is None or (is_finite_num(peak_abs) and float(peak_abs) > float(cur.get(\"sigma_abs\", 0.0) or 0.0)):\n",
    "            snap[\"peak\"] = {\n",
    "                \"dt\": _dt_iso(peak_dt),\n",
    "                \"sigma_abs\": _json_safe(peak_abs),\n",
    "                \"sigma_signed\": _json_safe(peak_signed),\n",
    "                \"first_sign\": int(first_sign),\n",
    "            }\n",
    "\n",
    "    def finalize_pre_event(reason=\"window_end\"):\n",
    "        nonlocal pre_active, pre_id\n",
    "        nonlocal global_norm_peak_sum, global_norm_peak_cnt\n",
    "\n",
    "        if not pre_active:\n",
    "            return\n",
    "\n",
    "        blue_status  = classify_blue()\n",
    "        ark_status   = classify_ark()\n",
    "        print_status = classify_print_with_frozen_peak(print_first_val)\n",
    "        open_status  = classify_open()\n",
    "\n",
    "        global_label = compute_global_label(blue_status, ark_status, print_status, open_status)\n",
    "        if global_label.endswith(\"_HARD\"):\n",
    "            global_kind = \"hard\"\n",
    "        elif global_label.endswith(\"_SOFT\"):\n",
    "            global_kind = \"soft\"\n",
    "        else:\n",
    "            global_kind = \"none\"\n",
    "\n",
    "        sk = pre_sign_key()\n",
    "\n",
    "        # OLD start band counters (kept) — any/hard/soft across all pre classes (OR semantics)\n",
    "        sb_total = floor_to_band(pre_start_dt, start_band_minutes)\n",
    "        if sb_total:\n",
    "            start_bands_pre_total[sb_total] += 1\n",
    "            if (blue_status != \"none\") or (ark_status != \"none\") or (print_status != \"none\") or (open_status != \"none\"):\n",
    "                start_bands_pre_any[sb_total] += 1\n",
    "            if (blue_status == \"hard\") or (ark_status == \"hard\") or (print_status == \"hard\") or (open_status == \"hard\"):\n",
    "                start_bands_pre_hard[sb_total] += 1\n",
    "            if (blue_status == \"soft\") or (ark_status == \"soft\") or (print_status == \"soft\") or (open_status == \"soft\"):\n",
    "                start_bands_pre_soft[sb_total] += 1\n",
    "\n",
    "        # NEW per-class-sign start band counters\n",
    "        if sb_total:\n",
    "            update_timeband_by_class_sign(timebands_pre_by_class_sign, \"blue\", \"start\", sk, sb_total, blue_status)\n",
    "            update_timeband_by_class_sign(timebands_pre_by_class_sign, \"ark\", \"start\", sk, sb_total, ark_status)\n",
    "            update_timeband_by_class_sign(timebands_pre_by_class_sign, \"print\", \"start\", sk, sb_total, print_status)\n",
    "            update_timeband_by_class_sign(timebands_pre_by_class_sign, \"open\", \"start\", sk, sb_total, open_status)\n",
    "            update_timeband_by_class_sign(timebands_pre_by_class_sign, \"global\", \"start\", sk, sb_total, global_kind)\n",
    "\n",
    "        # counts (classes independent)\n",
    "        counts_pre[\"blue\"][blue_status] += 1\n",
    "        counts_pre[\"ark\"][ark_status] += 1\n",
    "        counts_pre[\"print\"][print_status] += 1\n",
    "        counts_pre[\"open\"][open_status] += 1\n",
    "        counts_pre[\"global\"][global_kind] += 1\n",
    "        global_labels_counter[global_label] += 1\n",
    "\n",
    "        # HARD delays (kept + blue)\n",
    "        if blue_status == \"hard\":\n",
    "            add_hard_delay(\"blue\", pre_start_dt, blue_hard_dt)\n",
    "        if ark_status == \"hard\":\n",
    "            add_hard_delay(\"ark\", pre_start_dt, ark_hard_dt)\n",
    "        if print_status == \"hard\":\n",
    "            add_hard_delay(\"print\", pre_start_dt, print_first_dt)\n",
    "        if open_status == \"hard\":\n",
    "            add_hard_delay(\"open\", pre_start_dt, open_hard_dt)\n",
    "        if global_kind == \"hard\":\n",
    "            # delay is delay to the winning hard in GLOBAL\n",
    "            if global_label.startswith(\"BLUE_\"):\n",
    "                add_hard_delay(\"global\", pre_start_dt, blue_hard_dt)\n",
    "            elif global_label.startswith(\"ARK_\"):\n",
    "                add_hard_delay(\"global\", pre_start_dt, ark_hard_dt)\n",
    "            elif global_label.startswith(\"PRINT_\"):\n",
    "                add_hard_delay(\"global\", pre_start_dt, print_first_dt)\n",
    "            elif global_label.startswith(\"OPEN_\"):\n",
    "                add_hard_delay(\"global\", pre_start_dt, open_hard_dt)\n",
    "\n",
    "        # sigma bins:\n",
    "        # - BLUE uses BLUE peak (frozen to 03:59)\n",
    "        # - others use PRE peak (frozen to 09:29)\n",
    "        update_sigma_bins(sigma_bins_pre, \"blue\", sk, blue_peak_abs, blue_status)\n",
    "        update_sigma_bins(sigma_bins_pre, \"ark\", sk, pre_peak_abs, ark_status)\n",
    "        update_sigma_bins(sigma_bins_pre, \"print\", sk, pre_peak_abs, print_status)\n",
    "        update_sigma_bins(sigma_bins_pre, \"open\", sk, pre_peak_abs, open_status)\n",
    "        update_sigma_bins(sigma_bins_pre, \"global\", sk, pre_peak_abs, global_kind)  # global is modeled on PRE peak scale\n",
    "\n",
    "        # bench bins start+peak\n",
    "        update_bench_bins(bench_bins_pre, \"blue\", \"start\", sk, pre_start_bench, blue_status)\n",
    "        update_bench_bins(bench_bins_pre, \"blue\", \"peak\",  sk, blue_peak_bench, blue_status)\n",
    "\n",
    "        for cls, status in ((\"ark\", ark_status), (\"print\", print_status), (\"open\", open_status), (\"global\", global_kind)):\n",
    "            update_bench_bins(bench_bins_pre, cls, \"start\", sk, pre_start_bench, status)\n",
    "            update_bench_bins(bench_bins_pre, cls, \"peak\",  sk, pre_peak_bench, status)\n",
    "\n",
    "        # norm dt per class (for norm bench bins + norm timebands)\n",
    "        def cls_norm_dt(cls):\n",
    "            if cls == \"blue\":\n",
    "                return blue_hard_dt if blue_status == \"hard\" else (blue_soft_dt if blue_status == \"soft\" else None)\n",
    "            if cls == \"ark\":\n",
    "                return ark_hard_dt if ark_status == \"hard\" else (ark_soft_dt if ark_status == \"soft\" else None)\n",
    "            if cls == \"print\":\n",
    "                return print_first_dt if print_status != \"none\" else None\n",
    "            if cls == \"open\":\n",
    "                return open_hard_dt if open_status == \"hard\" else (open_soft_dt if open_status == \"soft\" else None)\n",
    "            if cls == \"global\":\n",
    "                if global_label.startswith(\"BLUE_\"):\n",
    "                    return cls_norm_dt(\"blue\")\n",
    "                if global_label.startswith(\"ARK_\"):\n",
    "                    return cls_norm_dt(\"ark\")\n",
    "                if global_label.startswith(\"PRINT_\"):\n",
    "                    return cls_norm_dt(\"print\")\n",
    "                if global_label.startswith(\"OPEN_\"):\n",
    "                    return cls_norm_dt(\"open\")\n",
    "                return None\n",
    "            return None\n",
    "\n",
    "        def cls_outcome(cls):\n",
    "            if cls == \"blue\": return blue_status\n",
    "            if cls == \"ark\": return ark_status\n",
    "            if cls == \"print\": return print_status\n",
    "            if cls == \"open\": return open_status\n",
    "            if cls == \"global\": return global_kind\n",
    "            return \"none\"\n",
    "\n",
    "        def cls_peak_signed(cls):\n",
    "            if cls == \"blue\": return blue_peak_signed\n",
    "            return pre_peak_signed\n",
    "\n",
    "        def cls_end_fields(cls, status):\n",
    "            if cls == \"blue\":\n",
    "                if status == \"hard\":\n",
    "                    return blue_hard_val, blue_hard_stock, blue_hard_bench\n",
    "                if status == \"soft\":\n",
    "                    return blue_soft_val, blue_soft_stock, blue_soft_bench\n",
    "                return np.nan, np.nan, np.nan\n",
    "            if cls == \"ark\":\n",
    "                if status == \"hard\":\n",
    "                    return ark_hard_val, ark_hard_stock, ark_hard_bench\n",
    "                if status == \"soft\":\n",
    "                    return ark_soft_val, ark_soft_stock, ark_soft_bench\n",
    "                return np.nan, np.nan, np.nan\n",
    "            if cls == \"print\":\n",
    "                if status != \"none\":\n",
    "                    return print_first_val, print_first_stock, print_first_bench\n",
    "                return np.nan, np.nan, np.nan\n",
    "            if cls == \"open\":\n",
    "                if status == \"hard\":\n",
    "                    return open_hard_val, open_hard_stock, open_hard_bench\n",
    "                if status == \"soft\":\n",
    "                    return open_soft_val, open_soft_stock, open_soft_bench\n",
    "                return np.nan, np.nan, np.nan\n",
    "            if cls == \"global\":\n",
    "                if global_label.startswith(\"BLUE_\"):\n",
    "                    return cls_end_fields(\"blue\", blue_status)\n",
    "                if global_label.startswith(\"ARK_\"):\n",
    "                    return cls_end_fields(\"ark\", ark_status)\n",
    "                if global_label.startswith(\"PRINT_\"):\n",
    "                    return cls_end_fields(\"print\", print_status)\n",
    "                if global_label.startswith(\"OPEN_\"):\n",
    "                    return cls_end_fields(\"open\", open_status)\n",
    "                return np.nan, np.nan, np.nan\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "        # norm bins + bench norm + ✅ last3 pushes (hard/soft only)\n",
    "        for cls in (\"blue\", \"ark\", \"print\", \"open\", \"global\"):\n",
    "            ndt = cls_norm_dt(cls)\n",
    "            status = cls_outcome(cls)\n",
    "            if isinstance(ndt, datetime):\n",
    "                b = floor_to_band(ndt, norm_band_minutes)\n",
    "                if b:\n",
    "                    # OLD norm counters kept (any/hard/soft only)\n",
    "                    if status != \"none\":\n",
    "                        norm_bands_pre_any[b] += 1\n",
    "                    if status == \"hard\":\n",
    "                        norm_bands_pre_hard[b] += 1\n",
    "                    if status == \"soft\":\n",
    "                        norm_bands_pre_soft[b] += 1\n",
    "\n",
    "                    # NEW per-class-sign norm bins\n",
    "                    update_timeband_by_class_sign(timebands_pre_by_class_sign, cls, \"norm\", sk, b, status)\n",
    "\n",
    "                    # bench norm bin value:\n",
    "                    if cls == \"blue\":\n",
    "                        end_bench_val = blue_hard_bench if status == \"hard\" else (blue_soft_bench if status == \"soft\" else np.nan)\n",
    "                        update_bench_bins(bench_bins_pre, cls, \"norm\", sk, end_bench_val, status)\n",
    "                    else:\n",
    "                        update_bench_bins(bench_bins_pre, cls, \"norm\", sk, pre_peak_bench, status)\n",
    "\n",
    "                # last3 only for normalized statuses\n",
    "                if status in (\"hard\", \"soft\"):\n",
    "                    end_dev, end_stock, end_bench = cls_end_fields(cls, status)\n",
    "                    push_last3_example(\n",
    "                        cls, sk, status,\n",
    "                        pre_start_dt, ndt,\n",
    "                        pre_start_dev, end_dev, cls_peak_signed(cls),\n",
    "                        pre_start_stock, end_stock,\n",
    "                        pre_start_bench, end_bench,\n",
    "                        start_time=pre_start_dt.strftime(\"%H:%M\") if isinstance(pre_start_dt, datetime) else None,\n",
    "                        end_time=ndt.strftime(\"%H:%M\") if isinstance(ndt, datetime) else None,\n",
    "                    )\n",
    "\n",
    "        # global mean peak for normalized — use BLUE peak if global label is BLUE\n",
    "        if global_kind != \"none\":\n",
    "            if global_label.startswith(\"BLUE_\") and is_finite_num(blue_peak_abs):\n",
    "                global_norm_peak_sum[sk] += float(blue_peak_abs)\n",
    "                global_norm_peak_cnt[sk] += 1\n",
    "            elif is_finite_num(pre_peak_abs):\n",
    "                global_norm_peak_sum[sk] += float(pre_peak_abs)\n",
    "                global_norm_peak_cnt[sk] += 1\n",
    "\n",
    "        # DAILY RECENT (kept): based on PRE peak (09:29-frozen)\n",
    "        if isinstance(pre_start_dt, datetime):\n",
    "            day_str = pre_start_dt.date().isoformat()\n",
    "            _ensure_recent_day(day_str)\n",
    "            _update_recent_daily_peak(day_str, pre_peak_dt, pre_peak_abs, pre_peak_signed, int(pre_start_sign))\n",
    "            if isinstance(print_first_dt, datetime) and is_finite_num(print_first_val):\n",
    "                _update_recent_daily_print(\n",
    "                    day_str,\n",
    "                    print_first_dt,\n",
    "                    float(print_first_val),\n",
    "                    float(pre_peak_abs),\n",
    "                    float(pre_peak_signed),\n",
    "                    int(pre_start_sign),\n",
    "                )\n",
    "\n",
    "        # rebuild last5 deques from recent_days (kept)\n",
    "        last5_print_days_pos.clear()\n",
    "        last5_print_days_neg.clear()\n",
    "        last5_peak_days_pos.clear()\n",
    "        last5_peak_days_neg.clear()\n",
    "        for d in list(recent_days):\n",
    "            snap = recent_by_day.get(d)\n",
    "            if not snap:\n",
    "                continue\n",
    "            pk = snap.get(\"peak\")\n",
    "            pr = snap.get(\"print\")\n",
    "            if pk and is_finite_num(pk.get(\"sigma_abs\")):\n",
    "                if int(pk.get(\"first_sign\", 1)) > 0:\n",
    "                    last5_peak_days_pos.append(float(pk[\"sigma_abs\"]))\n",
    "                else:\n",
    "                    last5_peak_days_neg.append(float(pk[\"sigma_abs\"]))\n",
    "            if pr and is_finite_num(pr.get(\"dev\")):\n",
    "                if int(pr.get(\"first_sign\", 1)) > 0:\n",
    "                    last5_print_days_pos.append(float(pr[\"dev\"]))\n",
    "                else:\n",
    "                    last5_print_days_neg.append(float(pr[\"dev\"]))\n",
    "            if (\n",
    "                len(last5_print_days_pos) >= 5\n",
    "                and len(last5_print_days_neg) >= 5\n",
    "                and len(last5_peak_days_pos) >= 5\n",
    "                and len(last5_peak_days_neg) >= 5\n",
    "            ):\n",
    "                break\n",
    "\n",
    "        if include_events_pre:\n",
    "            pre_events_buf.append({\n",
    "                \"pre_id\": int(pre_id),\n",
    "                \"reason_finalized\": reason,\n",
    "                \"start\": {\"dt\": _dt_iso(pre_start_dt), \"dev\": _json_safe(pre_start_dev), \"sign\": int(pre_start_sign),\n",
    "                          \"stock_pct\": _json_safe(pre_start_stock), \"bench_pct\": _json_safe(pre_start_bench)},\n",
    "                \"pre_peak_frozen\": {\"dt\": _dt_iso(pre_peak_dt), \"abs\": _json_safe(pre_peak_abs), \"signed\": _json_safe(pre_peak_signed),\n",
    "                                    \"bin\": sigma_bin(pre_peak_abs),\n",
    "                                    \"stock_pct\": _json_safe(pre_peak_stock), \"bench_pct\": _json_safe(pre_peak_bench)},\n",
    "                \"blue_peak_frozen\": {\"dt\": _dt_iso(blue_peak_dt), \"abs\": _json_safe(blue_peak_abs), \"signed\": _json_safe(blue_peak_signed),\n",
    "                                     \"bin\": sigma_bin(blue_peak_abs),\n",
    "                                     \"stock_pct\": _json_safe(blue_peak_stock), \"bench_pct\": _json_safe(blue_peak_bench)},\n",
    "                \"blue\": {\"status\": blue_status},\n",
    "                \"ark\": {\"status\": ark_status},\n",
    "                \"print\": {\"status\": print_status},\n",
    "                \"open\": {\"status\": open_status},\n",
    "                \"global\": {\"label\": global_label, \"kind\": global_kind},\n",
    "            })\n",
    "\n",
    "        pre_id += 1\n",
    "        reset_pre_event()\n",
    "\n",
    "    # ---------------- INTRA event state ----------------\n",
    "    intra_active = False\n",
    "    intra_id = 0\n",
    "\n",
    "    intra_start_dt = None\n",
    "    intra_start_dev = np.nan\n",
    "    intra_start_sign = 0\n",
    "    intra_start_stock = np.nan\n",
    "    intra_start_bench = np.nan\n",
    "\n",
    "    intra_peak_abs = 0.0\n",
    "    intra_peak_signed = 0.0\n",
    "    intra_peak_dt = None\n",
    "    intra_peak_stock = np.nan\n",
    "    intra_peak_bench = np.nan\n",
    "\n",
    "    intra_hard_dt = None\n",
    "    intra_hard_val = np.nan\n",
    "    intra_hard_stock = np.nan\n",
    "    intra_hard_bench = np.nan\n",
    "\n",
    "    intra_soft_found = False\n",
    "    intra_soft_dt = None\n",
    "    intra_soft_val = np.nan\n",
    "    intra_soft_stock = np.nan\n",
    "    intra_soft_bench = np.nan\n",
    "\n",
    "    def reset_intra_event():\n",
    "        nonlocal intra_active, intra_start_dt, intra_start_dev, intra_start_sign, intra_start_stock, intra_start_bench\n",
    "        nonlocal intra_peak_abs, intra_peak_signed, intra_peak_dt, intra_peak_stock, intra_peak_bench\n",
    "        nonlocal intra_hard_dt, intra_hard_val, intra_hard_stock, intra_hard_bench\n",
    "        nonlocal intra_soft_found, intra_soft_dt, intra_soft_val, intra_soft_stock, intra_soft_bench\n",
    "\n",
    "        intra_active = False\n",
    "        intra_start_dt = None\n",
    "        intra_start_dev = np.nan\n",
    "        intra_start_sign = 0\n",
    "        intra_start_stock = np.nan\n",
    "        intra_start_bench = np.nan\n",
    "\n",
    "        intra_peak_abs = 0.0\n",
    "        intra_peak_signed = 0.0\n",
    "        intra_peak_dt = None\n",
    "        intra_peak_stock = np.nan\n",
    "        intra_peak_bench = np.nan\n",
    "\n",
    "        intra_hard_dt = None\n",
    "        intra_hard_val = np.nan\n",
    "        intra_hard_stock = np.nan\n",
    "        intra_hard_bench = np.nan\n",
    "\n",
    "        intra_soft_found = False\n",
    "        intra_soft_dt = None\n",
    "        intra_soft_val = np.nan\n",
    "        intra_soft_stock = np.nan\n",
    "        intra_soft_bench = np.nan\n",
    "\n",
    "    def start_intra_event(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal intra_active, intra_start_dt, intra_start_dev, intra_start_sign, intra_start_stock, intra_start_bench\n",
    "        nonlocal intra_peak_abs, intra_peak_signed, intra_peak_dt, intra_peak_stock, intra_peak_bench\n",
    "\n",
    "        intra_active = True\n",
    "        intra_start_dt = dt_now\n",
    "        intra_start_dev = float(dev_now)\n",
    "        intra_start_sign = 1 if float(dev_now) >= 0 else -1\n",
    "        intra_start_stock = stock_pct\n",
    "        intra_start_bench = bench_pct\n",
    "\n",
    "        intra_peak_abs = abs(float(dev_now))\n",
    "        intra_peak_signed = float(dev_now)\n",
    "        intra_peak_dt = dt_now\n",
    "        intra_peak_stock = stock_pct\n",
    "        intra_peak_bench = bench_pct\n",
    "\n",
    "    def intra_sign_key():\n",
    "        return \"pos\" if intra_start_sign > 0 else \"neg\"\n",
    "\n",
    "    def intra_process_tick(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal intra_peak_abs, intra_peak_signed, intra_peak_dt, intra_peak_stock, intra_peak_bench\n",
    "        nonlocal intra_hard_dt, intra_hard_val, intra_hard_stock, intra_hard_bench\n",
    "        nonlocal intra_soft_found, intra_soft_dt, intra_soft_val, intra_soft_stock, intra_soft_bench\n",
    "\n",
    "        cur_abs = abs(float(dev_now))\n",
    "\n",
    "        if cur_abs > float(intra_peak_abs):\n",
    "            intra_peak_abs = float(cur_abs)\n",
    "            intra_peak_signed = float(dev_now)\n",
    "            intra_peak_dt = dt_now\n",
    "            intra_peak_stock = stock_pct\n",
    "            intra_peak_bench = bench_pct\n",
    "            if intra_hard_dt is None:\n",
    "                intra_soft_found = False\n",
    "                intra_soft_dt = None\n",
    "                intra_soft_val = np.nan\n",
    "                intra_soft_stock = np.nan\n",
    "                intra_soft_bench = np.nan\n",
    "\n",
    "        t = (dt_now.hour, dt_now.minute)\n",
    "        if not in_range(t, INTRA_FROM, INTRA_TO):\n",
    "            return\n",
    "\n",
    "        if intra_hard_dt is None and cur_abs <= norm_thr:\n",
    "            intra_hard_dt = dt_now\n",
    "            intra_hard_val = float(dev_now)\n",
    "            intra_hard_stock = stock_pct\n",
    "            intra_hard_bench = bench_pct\n",
    "\n",
    "        if intra_hard_dt is None and isinstance(intra_peak_dt, datetime) and intra_peak_abs > 0:\n",
    "            if dt_now >= intra_peak_dt and cur_abs <= (float(intra_peak_abs) / float(soft_ratio)):\n",
    "                if (not intra_soft_found) or (dt_now < intra_soft_dt):\n",
    "                    intra_soft_found = True\n",
    "                    intra_soft_dt = dt_now\n",
    "                    intra_soft_val = float(dev_now)\n",
    "                    intra_soft_stock = stock_pct\n",
    "                    intra_soft_bench = bench_pct\n",
    "\n",
    "    def finalize_intra_event(reason=\"window_end\"):\n",
    "        nonlocal intra_active, intra_id\n",
    "        if not intra_active:\n",
    "            return\n",
    "\n",
    "        if intra_hard_dt is not None:\n",
    "            status = \"hard\"\n",
    "            end_dt = intra_hard_dt\n",
    "            add_hard_delay(\"intra\", intra_start_dt, intra_hard_dt)\n",
    "        elif intra_soft_found and intra_soft_dt is not None:\n",
    "            status = \"soft\"\n",
    "            end_dt = intra_soft_dt\n",
    "        else:\n",
    "            status = \"none\"\n",
    "            end_dt = None\n",
    "\n",
    "        sk = intra_sign_key()\n",
    "\n",
    "        sb = floor_to_band(intra_start_dt, start_band_minutes)\n",
    "        if sb:\n",
    "            start_bands_intra_total[sb] += 1\n",
    "            if status != \"none\":\n",
    "                start_bands_intra_any[sb] += 1\n",
    "            if status == \"hard\":\n",
    "                start_bands_intra_hard[sb] += 1\n",
    "            if status == \"soft\":\n",
    "                start_bands_intra_soft[sb] += 1\n",
    "\n",
    "            update_timeband_by_class_sign(timebands_intra_by_class_sign, \"intra\", \"start\", sk, sb, status)\n",
    "\n",
    "        counts_intra[\"intra\"][status] += 1\n",
    "\n",
    "        update_sigma_bins(sigma_bins_intra, \"intra\", sk, intra_peak_abs, status)\n",
    "        update_bench_bins(bench_bins_intra, \"intra\", \"start\", sk, intra_start_bench, status)\n",
    "        update_bench_bins(bench_bins_intra, \"intra\", \"peak\",  sk, intra_peak_bench, status)\n",
    "\n",
    "        if status != \"none\" and isinstance(end_dt, datetime):\n",
    "            b = floor_to_band(end_dt, norm_band_minutes)\n",
    "            if b:\n",
    "                norm_bands_intra_any[b] += 1\n",
    "                if status == \"hard\":\n",
    "                    norm_bands_intra_hard[b] += 1\n",
    "                if status == \"soft\":\n",
    "                    norm_bands_intra_soft[b] += 1\n",
    "\n",
    "                update_timeband_by_class_sign(timebands_intra_by_class_sign, \"intra\", \"norm\", sk, b, status)\n",
    "                update_bench_bins(bench_bins_intra, \"intra\", \"norm\", sk, (intra_hard_bench if status == \"hard\" else intra_soft_bench), status)\n",
    "\n",
    "            push_last3_example(\n",
    "                \"intra\", sk, status,\n",
    "                intra_start_dt, end_dt,\n",
    "                intra_start_dev, (intra_hard_val if status == \"hard\" else intra_soft_val),\n",
    "                intra_peak_signed,\n",
    "                intra_start_stock, (intra_hard_stock if status == \"hard\" else intra_soft_stock),\n",
    "                intra_start_bench, (intra_hard_bench if status == \"hard\" else intra_soft_bench),\n",
    "                start_time=intra_start_dt.strftime(\"%H:%M\") if isinstance(intra_start_dt, datetime) else None,\n",
    "                end_time=end_dt.strftime(\"%H:%M\") if isinstance(end_dt, datetime) else None,\n",
    "            )\n",
    "\n",
    "        if include_events_intra:\n",
    "            intra_events_buf.append({\n",
    "                \"intra_id\": int(intra_id),\n",
    "                \"reason_finalized\": reason,\n",
    "                \"start\": {\"dt\": _dt_iso(intra_start_dt), \"dev\": _json_safe(intra_start_dev), \"sign\": int(intra_start_sign)},\n",
    "                \"peak\":  {\"dt\": _dt_iso(intra_peak_dt), \"abs\": _json_safe(intra_peak_abs), \"signed\": _json_safe(intra_peak_signed)},\n",
    "                \"status\": status,\n",
    "            })\n",
    "\n",
    "        intra_id += 1\n",
    "        reset_intra_event()\n",
    "\n",
    "    # ---------------- POST event state ----------------\n",
    "    post_active = False\n",
    "    post_id = 0\n",
    "\n",
    "    post_start_dt = None\n",
    "    post_start_dev = np.nan\n",
    "    post_start_sign = 0\n",
    "    post_start_stock = np.nan\n",
    "    post_start_bench = np.nan\n",
    "\n",
    "    post_peak_abs = 0.0\n",
    "    post_peak_signed = 0.0\n",
    "    post_peak_dt = None\n",
    "    post_peak_stock = np.nan\n",
    "    post_peak_bench = np.nan\n",
    "\n",
    "    post_hard_dt = None\n",
    "    post_hard_val = np.nan\n",
    "    post_hard_stock = np.nan\n",
    "    post_hard_bench = np.nan\n",
    "\n",
    "    post_soft_found = False\n",
    "    post_soft_dt = None\n",
    "    post_soft_val = np.nan\n",
    "    post_soft_stock = np.nan\n",
    "    post_soft_bench = np.nan\n",
    "\n",
    "    def reset_post_event():\n",
    "        nonlocal post_active, post_start_dt, post_start_dev, post_start_sign, post_start_stock, post_start_bench\n",
    "        nonlocal post_peak_abs, post_peak_signed, post_peak_dt, post_peak_stock, post_peak_bench\n",
    "        nonlocal post_hard_dt, post_hard_val, post_hard_stock, post_hard_bench\n",
    "        nonlocal post_soft_found, post_soft_dt, post_soft_val, post_soft_stock, post_soft_bench\n",
    "\n",
    "        post_active = False\n",
    "        post_start_dt = None\n",
    "        post_start_dev = np.nan\n",
    "        post_start_sign = 0\n",
    "        post_start_stock = np.nan\n",
    "        post_start_bench = np.nan\n",
    "\n",
    "        post_peak_abs = 0.0\n",
    "        post_peak_signed = 0.0\n",
    "        post_peak_dt = None\n",
    "        post_peak_stock = np.nan\n",
    "        post_peak_bench = np.nan\n",
    "\n",
    "        post_hard_dt = None\n",
    "        post_hard_val = np.nan\n",
    "        post_hard_stock = np.nan\n",
    "        post_hard_bench = np.nan\n",
    "\n",
    "        post_soft_found = False\n",
    "        post_soft_dt = None\n",
    "        post_soft_val = np.nan\n",
    "        post_soft_stock = np.nan\n",
    "        post_soft_bench = np.nan\n",
    "\n",
    "    def start_post_event(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal post_active, post_start_dt, post_start_dev, post_start_sign, post_start_stock, post_start_bench\n",
    "        nonlocal post_peak_abs, post_peak_signed, post_peak_dt, post_peak_stock, post_peak_bench\n",
    "\n",
    "        post_active = True\n",
    "        post_start_dt = dt_now\n",
    "        post_start_dev = float(dev_now)\n",
    "        post_start_sign = 1 if float(dev_now) >= 0 else -1\n",
    "        post_start_stock = stock_pct\n",
    "        post_start_bench = bench_pct\n",
    "\n",
    "        post_peak_abs = abs(float(dev_now))\n",
    "        post_peak_signed = float(dev_now)\n",
    "        post_peak_dt = dt_now\n",
    "        post_peak_stock = stock_pct\n",
    "        post_peak_bench = bench_pct\n",
    "\n",
    "    def post_sign_key():\n",
    "        return \"pos\" if post_start_sign > 0 else \"neg\"\n",
    "\n",
    "    def post_process_tick(dt_now, dev_now, stock_pct, bench_pct):\n",
    "        nonlocal post_peak_abs, post_peak_signed, post_peak_dt, post_peak_stock, post_peak_bench\n",
    "        nonlocal post_hard_dt, post_hard_val, post_hard_stock, post_hard_bench\n",
    "        nonlocal post_soft_found, post_soft_dt, post_soft_val, post_soft_stock, post_soft_bench\n",
    "\n",
    "        cur_abs = abs(float(dev_now))\n",
    "\n",
    "        if cur_abs > float(post_peak_abs):\n",
    "            post_peak_abs = float(cur_abs)\n",
    "            post_peak_signed = float(dev_now)\n",
    "            post_peak_dt = dt_now\n",
    "            post_peak_stock = stock_pct\n",
    "            post_peak_bench = bench_pct\n",
    "            if post_hard_dt is None:\n",
    "                post_soft_found = False\n",
    "                post_soft_dt = None\n",
    "                post_soft_val = np.nan\n",
    "                post_soft_stock = np.nan\n",
    "                post_soft_bench = np.nan\n",
    "\n",
    "        t = (dt_now.hour, dt_now.minute)\n",
    "        if not in_range(t, POST_FROM, POST_TO):\n",
    "            return\n",
    "\n",
    "        if post_hard_dt is None and cur_abs <= norm_thr:\n",
    "            post_hard_dt = dt_now\n",
    "            post_hard_val = float(dev_now)\n",
    "            post_hard_stock = stock_pct\n",
    "            post_hard_bench = bench_pct\n",
    "\n",
    "        if post_hard_dt is None and isinstance(post_peak_dt, datetime) and post_peak_abs > 0:\n",
    "            if dt_now >= post_peak_dt and cur_abs <= (float(post_peak_abs) / float(soft_ratio)):\n",
    "                if (not post_soft_found) or (dt_now < post_soft_dt):\n",
    "                    post_soft_found = True\n",
    "                    post_soft_dt = dt_now\n",
    "                    post_soft_val = float(dev_now)\n",
    "                    post_soft_stock = stock_pct\n",
    "                    post_soft_bench = bench_pct\n",
    "\n",
    "    def finalize_post_event(reason=\"window_end\"):\n",
    "        nonlocal post_active, post_id\n",
    "        if not post_active:\n",
    "            return\n",
    "\n",
    "        if post_hard_dt is not None:\n",
    "            status = \"hard\"\n",
    "            end_dt = post_hard_dt\n",
    "            add_hard_delay(\"post\", post_start_dt, post_hard_dt)\n",
    "        elif post_soft_found and post_soft_dt is not None:\n",
    "            status = \"soft\"\n",
    "            end_dt = post_soft_dt\n",
    "        else:\n",
    "            status = \"none\"\n",
    "            end_dt = None\n",
    "\n",
    "        sk = post_sign_key()\n",
    "\n",
    "        sb = floor_to_band(post_start_dt, start_band_minutes)\n",
    "        if sb:\n",
    "            start_bands_post_total[sb] += 1\n",
    "            if status != \"none\":\n",
    "                start_bands_post_any[sb] += 1\n",
    "            if status == \"hard\":\n",
    "                start_bands_post_hard[sb] += 1\n",
    "            if status == \"soft\":\n",
    "                start_bands_post_soft[sb] += 1\n",
    "\n",
    "            update_timeband_by_class_sign(timebands_post_by_class_sign, \"post\", \"start\", sk, sb, status)\n",
    "\n",
    "        counts_post[\"post\"][status] += 1\n",
    "\n",
    "        update_sigma_bins(sigma_bins_post, \"post\", sk, post_peak_abs, status)\n",
    "        update_bench_bins(bench_bins_post, \"post\", \"start\", sk, post_start_bench, status)\n",
    "        update_bench_bins(bench_bins_post, \"post\", \"peak\",  sk, post_peak_bench, status)\n",
    "\n",
    "        if status != \"none\" and isinstance(end_dt, datetime):\n",
    "            b = floor_to_band(end_dt, norm_band_minutes)\n",
    "            if b:\n",
    "                norm_bands_post_any[b] += 1\n",
    "                if status == \"hard\":\n",
    "                    norm_bands_post_hard[b] += 1\n",
    "                if status == \"soft\":\n",
    "                    norm_bands_post_soft[b] += 1\n",
    "\n",
    "                update_timeband_by_class_sign(timebands_post_by_class_sign, \"post\", \"norm\", sk, b, status)\n",
    "                update_bench_bins(bench_bins_post, \"post\", \"norm\", sk, (post_hard_bench if status == \"hard\" else post_soft_bench), status)\n",
    "\n",
    "            push_last3_example(\n",
    "                \"post\", sk, status,\n",
    "                post_start_dt, end_dt,\n",
    "                post_start_dev, (post_hard_val if status == \"hard\" else post_soft_val),\n",
    "                post_peak_signed,\n",
    "                post_start_stock, (post_hard_stock if status == \"hard\" else post_soft_stock),\n",
    "                post_start_bench, (post_hard_bench if status == \"hard\" else post_soft_bench),\n",
    "                start_time=post_start_dt.strftime(\"%H:%M\") if isinstance(post_start_dt, datetime) else None,\n",
    "                end_time=end_dt.strftime(\"%H:%M\") if isinstance(end_dt, datetime) else None,\n",
    "            )\n",
    "\n",
    "        if include_events_post:\n",
    "            post_events_buf.append({\n",
    "                \"post_id\": int(post_id),\n",
    "                \"reason_finalized\": reason,\n",
    "                \"start\": {\"dt\": _dt_iso(post_start_dt), \"dev\": _json_safe(post_start_dev), \"sign\": int(post_start_sign)},\n",
    "                \"peak\":  {\"dt\": _dt_iso(post_peak_dt), \"abs\": _json_safe(post_peak_abs), \"signed\": _json_safe(post_peak_signed)},\n",
    "                \"status\": status,\n",
    "            })\n",
    "\n",
    "        post_id += 1\n",
    "        reset_post_event()\n",
    "\n",
    "    # ---------------- day boundary helpers ----------------\n",
    "    def on_new_day():\n",
    "        if pre_active:\n",
    "            finalize_pre_event(reason=\"day_boundary\")\n",
    "        if intra_active:\n",
    "            finalize_intra_event(reason=\"day_boundary\")\n",
    "        if post_active:\n",
    "            finalize_post_event(reason=\"day_boundary\")\n",
    "\n",
    "    # ---------------- dictify helpers ----------------\n",
    "    def dictify_sigma_bins(m):\n",
    "        return {\n",
    "            \"pos\": {b: dict(c) for b, c in m[\"pos\"].items()},\n",
    "            \"neg\": {b: dict(c) for b, c in m[\"neg\"].items()},\n",
    "        }\n",
    "\n",
    "    def dictify_bench_bins(m):\n",
    "        out = {}\n",
    "        for which in (\"start\", \"peak\", \"norm\"):\n",
    "            out[which] = {\n",
    "                \"pos\": {b: dict(c) for b, c in m[which][\"pos\"].items()},\n",
    "                \"neg\": {b: dict(c) for b, c in m[which][\"neg\"].items()},\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    def dictify_timebands_by_class_sign(m):\n",
    "        out = {}\n",
    "        for cls, blk in m.items():\n",
    "            out[cls] = {}\n",
    "            for which in (\"start\", \"norm\"):\n",
    "                out[cls][which] = {\n",
    "                    \"pos\": {band: dict(c) for band, c in blk[which][\"pos\"].items()},\n",
    "                    \"neg\": {band: dict(c) for band, c in blk[which][\"neg\"].items()},\n",
    "                }\n",
    "        return out\n",
    "\n",
    "    def dictify_last3(last3_map):\n",
    "        out = {}\n",
    "        for cls, by_sign in last3_map.items():\n",
    "            out[cls] = {\"pos\": list(by_sign[\"pos\"]), \"neg\": list(by_sign[\"neg\"])}\n",
    "        return out\n",
    "\n",
    "    # ---------------- flush ticker (write files) ----------------\n",
    "    def flush_current_ticker():\n",
    "        nonlocal cur_ticker, cur_day\n",
    "        nonlocal bench_name_seen, corr_static, beta_static, sigma_static\n",
    "\n",
    "        if cur_ticker is None:\n",
    "            return\n",
    "\n",
    "        if pre_active:\n",
    "            finalize_pre_event(reason=\"ticker_end\")\n",
    "        if intra_active:\n",
    "            finalize_intra_event(reason=\"ticker_end\")\n",
    "        if post_active:\n",
    "            finalize_post_event(reason=\"ticker_end\")\n",
    "\n",
    "        blue_r = class_rates(counts_pre[\"blue\"])\n",
    "        ark_r = class_rates(counts_pre[\"ark\"])\n",
    "        pr_r  = class_rates(counts_pre[\"print\"])\n",
    "        op_r  = class_rates(counts_pre[\"open\"])\n",
    "        gl_r  = class_rates(counts_pre[\"global\"])\n",
    "        intra_r = class_rates(counts_intra[\"intra\"])\n",
    "        post_r  = class_rates(counts_post[\"post\"])\n",
    "\n",
    "        events_pre_total = int(gl_r[\"total\"])  # pre event count aligns with global counter\n",
    "        events_intra_total = int(intra_r[\"total\"])\n",
    "        events_post_total  = int(post_r[\"total\"])\n",
    "        events_total = events_pre_total + events_intra_total + events_post_total\n",
    "\n",
    "        # ✅ global filter for ALL outputs\n",
    "        if events_total < int(min_events_per_ticker):\n",
    "            reset_ticker_state()\n",
    "            return\n",
    "\n",
    "        # last10\n",
    "        last10_print_days = []\n",
    "        last10_peak_days = []\n",
    "        for d in list(recent_days):\n",
    "            snap = recent_by_day.get(d)\n",
    "            if not snap:\n",
    "                continue\n",
    "            if snap.get(\"print\") is not None:\n",
    "                last10_print_days.append(snap[\"print\"])\n",
    "            if snap.get(\"peak\") is not None:\n",
    "                last10_peak_days.append(snap[\"peak\"])\n",
    "\n",
    "        pos_vals = list(last5_print_days_pos)\n",
    "        neg_vals = list(last5_print_days_neg)\n",
    "\n",
    "        # open series last10\n",
    "        open_series_last10 = []\n",
    "        for d in list(recent_days):\n",
    "            series_map = open_series_by_day.get(d)\n",
    "            if not series_map:\n",
    "                continue\n",
    "            pts = sorted(series_map.items(), key=lambda kv: kv[0])\n",
    "            open_series_last10.append({\n",
    "                \"date\": d,\n",
    "                \"points\": [[dt_iso, _json_safe(val)] for dt_iso, val in pts],\n",
    "            })\n",
    "\n",
    "        payload = {\n",
    "            \"ticker\": cur_ticker,\n",
    "            \"bench\": bench_name_seen,\n",
    "            \"static\": {\"corr\": _json_safe(corr_static), \"beta\": _json_safe(beta_static), \"sigma\": _json_safe(sigma_static)},\n",
    "            \"params\": {\n",
    "                \"dev_thr\": float(dev_thr), \"norm_thr\": float(norm_thr), \"soft_ratio\": float(soft_ratio),\n",
    "                \"windows\": {\n",
    "                    \"blue\": \"00:01-03:59 (trigger allowed; peak frozen to 03:59; hard/soft within BLUE)\",\n",
    "                    \"fixation_window\": \"00:05-09:29 (ARK peak frozen; ARK hard/soft within)\",\n",
    "                    \"ignored_gaps\": [\"03:58-04:05\", \"07:58-08:05\"],\n",
    "                    \"frozen_peak_until\": \"09:29\",\n",
    "                    \"print_first\": \"09:30-09:35 (first tick only)\",\n",
    "                    \"open_scan\": \"09:31-09:40 (scan + open dev series)\",\n",
    "                    \"intra\": \"10:00-12:00 (trigger+normalize within)\",\n",
    "                    \"post\": \"16:01-19:59 (trigger+normalize within)\",\n",
    "                    \"global_priority\": GLOBAL_PRIORITY,\n",
    "                },\n",
    "                \"bins\": {\n",
    "                    \"sigma\": {\"min\": sigma_bin_min, \"max\": sigma_bin_max, \"step\": sigma_bin_step},\n",
    "                    \"bench\": {\"min\": bench_bin_min, \"max\": bench_bin_max, \"step\": bench_bin_step},\n",
    "                },\n",
    "                \"time_bands\": {\"start_band_minutes\": start_band_minutes, \"norm_band_minutes\": norm_band_minutes},\n",
    "                \"best_rules\": best_rules,\n",
    "                \"min_events_per_ticker\": int(min_events_per_ticker),\n",
    "                \"open_series_downsample_seconds\": int(open_series_downsample_seconds),\n",
    "            },\n",
    "      \"stats\": {\n",
    "                \"events_total\": int(events_total),\n",
    "                \"pre\": {\n",
    "                    \"events_total\": int(events_pre_total),\n",
    "                    \"blue\": blue_r,\n",
    "                    \"ark\": ark_r,\n",
    "                    \"print\": pr_r,\n",
    "                    \"open\": op_r,\n",
    "                    \"global\": {\n",
    "                        **gl_r,\n",
    "                        \"labels\": dict(global_labels_counter),\n",
    "                        \"best_label\": global_labels_counter.most_common(1)[0][0] if global_labels_counter else None,\n",
    "                    },\n",
    "                    \"hard_delay_avg_sec\": {\n",
    "                        \"blue\": _json_safe(avg_hard_delay(\"blue\")),\n",
    "                        \"ark\": _json_safe(avg_hard_delay(\"ark\")),\n",
    "                        \"print\": _json_safe(avg_hard_delay(\"print\")),\n",
    "                        \"open\": _json_safe(avg_hard_delay(\"open\")),\n",
    "                        \"global\": _json_safe(avg_hard_delay(\"global\")),\n",
    "                    },\n",
    "                    \"global_mean_peak_abs_when_normalized\": {\n",
    "                        \"pos\": _json_safe((global_norm_peak_sum[\"pos\"] / global_norm_peak_cnt[\"pos\"]) if global_norm_peak_cnt[\"pos\"] else None),\n",
    "                        \"neg\": _json_safe((global_norm_peak_sum[\"neg\"] / global_norm_peak_cnt[\"neg\"]) if global_norm_peak_cnt[\"neg\"] else None),\n",
    "                    },\n",
    "                },\n",
    "                \"intra\": {\n",
    "                    \"events_total\": int(events_intra_total),\n",
    "                    \"intra\": intra_r,\n",
    "                    \"hard_delay_avg_sec\": {\"intra\": _json_safe(avg_hard_delay(\"intra\"))},\n",
    "                },\n",
    "                \"post\": {\n",
    "                    \"events_total\": int(events_post_total),\n",
    "                    \"post\": post_r,\n",
    "                    \"hard_delay_avg_sec\": {\"post\": _json_safe(avg_hard_delay(\"post\"))},\n",
    "                },\n",
    "            },\n",
    "            \"time_bands\": {\n",
    "                \"pre\": {\n",
    "                    \"start_total\": dict(start_bands_pre_total),\n",
    "                    \"start_any\": dict(start_bands_pre_any),\n",
    "                    \"start_hard\": dict(start_bands_pre_hard),\n",
    "                    \"start_soft\": dict(start_bands_pre_soft),\n",
    "                    \"norm_any\": dict(norm_bands_pre_any),\n",
    "                    \"norm_hard\": dict(norm_bands_pre_hard),\n",
    "                    \"norm_soft\": dict(norm_bands_pre_soft),\n",
    "                },\n",
    "                \"intra\": {\n",
    "                    \"start_total\": dict(start_bands_intra_total),\n",
    "                    \"start_any\": dict(start_bands_intra_any),\n",
    "                    \"start_hard\": dict(start_bands_intra_hard),\n",
    "                    \"start_soft\": dict(start_bands_intra_soft),\n",
    "                    \"norm_any\": dict(norm_bands_intra_any),\n",
    "                    \"norm_hard\": dict(norm_bands_intra_hard),\n",
    "                    \"norm_soft\": dict(norm_bands_intra_soft),\n",
    "                },\n",
    "                \"post\": {\n",
    "                    \"start_total\": dict(start_bands_post_total),\n",
    "                    \"start_any\": dict(start_bands_post_any),\n",
    "                    \"start_hard\": dict(start_bands_post_hard),\n",
    "                    \"start_soft\": dict(start_bands_post_soft),\n",
    "                    \"norm_any\": dict(norm_bands_post_any),\n",
    "                    \"norm_hard\": dict(norm_bands_post_hard),\n",
    "                    \"norm_soft\": dict(norm_bands_post_soft),\n",
    "                },\n",
    "                \"pre_by_class_sign\": dictify_timebands_by_class_sign(timebands_pre_by_class_sign),\n",
    "                \"intra_by_class_sign\": dictify_timebands_by_class_sign(timebands_intra_by_class_sign),\n",
    "                \"post_by_class_sign\": dictify_timebands_by_class_sign(timebands_post_by_class_sign),\n",
    "            },\n",
    "            \"recent\": {\n",
    "                \"last10_days\": list(recent_days),\n",
    "                \"last10_print\": last10_print_days,\n",
    "                \"last10_pre_peak_sigma\": last10_peak_days,\n",
    "                \"last10_open_dev_series\": open_series_last10,\n",
    "                \"last5_print\": {\n",
    "                    \"pos\": {\n",
    "                        \"values\": pos_vals,\n",
    "                        \"mean\": _json_safe(float(np.mean(pos_vals)) if pos_vals else None),\n",
    "                        \"median\": _json_safe(float(np.median(pos_vals)) if pos_vals else None),\n",
    "                    },\n",
    "                    \"neg\": {\n",
    "                        \"values\": neg_vals,\n",
    "                        \"mean\": _json_safe(float(np.mean(neg_vals)) if neg_vals else None),\n",
    "                        \"median\": _json_safe(float(np.median(neg_vals)) if neg_vals else None),\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "\n",
    "            \"examples_last3_normalized\": dictify_last3(last3_examples),\n",
    "\n",
    "            \"bins\": {\n",
    "                \"sigma\": {\n",
    "                    \"pre\": {\n",
    "                        \"blue\": dictify_sigma_bins(sigma_bins_pre[\"blue\"]),\n",
    "                        \"ark\": dictify_sigma_bins(sigma_bins_pre[\"ark\"]),\n",
    "                        \"print\": dictify_sigma_bins(sigma_bins_pre[\"print\"]),\n",
    "                        \"open\": dictify_sigma_bins(sigma_bins_pre[\"open\"]),\n",
    "                        \"global\": dictify_sigma_bins(sigma_bins_pre[\"global\"]),\n",
    "                    },\n",
    "                    \"intra\": {\"intra\": dictify_sigma_bins(sigma_bins_intra[\"intra\"])},\n",
    "                    \"post\": {\"post\": dictify_sigma_bins(sigma_bins_post[\"post\"])},\n",
    "                },\n",
    "                \"bench\": {\n",
    "                    \"pre\": {\n",
    "                        \"blue\": dictify_bench_bins(bench_bins_pre[\"blue\"]),\n",
    "                        \"ark\": dictify_bench_bins(bench_bins_pre[\"ark\"]),\n",
    "                        \"print\": dictify_bench_bins(bench_bins_pre[\"print\"]),\n",
    "                        \"open\": dictify_bench_bins(bench_bins_pre[\"open\"]),\n",
    "                        \"global\": dictify_bench_bins(bench_bins_pre[\"global\"]),\n",
    "                    },\n",
    "                    \"intra\": {\"intra\": dictify_bench_bins(bench_bins_intra[\"intra\"])},\n",
    "                    \"post\": {\"post\": dictify_bench_bins(bench_bins_post[\"post\"])},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if include_events_pre:\n",
    "            payload[\"events_pre\"] = list(pre_events_buf)\n",
    "        if include_events_intra:\n",
    "            payload[\"events_intra\"] = list(intra_events_buf)\n",
    "        if include_events_post:\n",
    "            payload[\"events_post\"] = list(post_events_buf)\n",
    "\n",
    "        onefile_f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # SUMMARY row\n",
    "        row = {\n",
    "            \"ticker\": cur_ticker,\n",
    "            \"bench\": bench_name_seen,\n",
    "            \"events_total\": int(events_total),\n",
    "            \"events_pre_total\": int(events_pre_total),\n",
    "            \"events_intra_total\": int(events_intra_total),\n",
    "            \"events_post_total\": int(events_post_total),\n",
    "\n",
    "            \"blue_any_rate\": _json_safe(blue_r[\"rate_any\"]),\n",
    "            \"blue_hard_rate\": _json_safe(blue_r[\"rate_hard\"]),\n",
    "            \"blue_soft_rate\": _json_safe(blue_r[\"rate_soft\"]),\n",
    "\n",
    "            \"ark_any_rate\": _json_safe(ark_r[\"rate_any\"]),\n",
    "            \"ark_hard_rate\": _json_safe(ark_r[\"rate_hard\"]),\n",
    "            \"ark_soft_rate\": _json_safe(ark_r[\"rate_soft\"]),\n",
    "\n",
    "            \"print_any_rate\": _json_safe(pr_r[\"rate_any\"]),\n",
    "            \"print_hard_rate\": _json_safe(pr_r[\"rate_hard\"]),\n",
    "            \"print_soft_rate\": _json_safe(pr_r[\"rate_soft\"]),\n",
    "\n",
    "            \"open_any_rate\": _json_safe(op_r[\"rate_any\"]),\n",
    "            \"open_hard_rate\": _json_safe(op_r[\"rate_hard\"]),\n",
    "            \"open_soft_rate\": _json_safe(op_r[\"rate_soft\"]),\n",
    "\n",
    "            \"global_any_rate\": _json_safe(gl_r[\"rate_any\"]),\n",
    "            \"global_hard_rate\": _json_safe(gl_r[\"rate_hard\"]),\n",
    "            \"global_soft_rate\": _json_safe(gl_r[\"rate_soft\"]),\n",
    "\n",
    "            \"intra_any_rate\": _json_safe(intra_r[\"rate_any\"]),\n",
    "            \"intra_hard_rate\": _json_safe(intra_r[\"rate_hard\"]),\n",
    "            \"intra_soft_rate\": _json_safe(intra_r[\"rate_soft\"]),\n",
    "\n",
    "            \"post_any_rate\": _json_safe(post_r[\"rate_any\"]),\n",
    "            \"post_hard_rate\": _json_safe(post_r[\"rate_hard\"]),\n",
    "            \"post_soft_rate\": _json_safe(post_r[\"rate_soft\"]),\n",
    "\n",
    "            \"corr\": _json_safe(corr_static),\n",
    "            \"beta\": _json_safe(beta_static),\n",
    "            \"sigma\": _json_safe(sigma_static),\n",
    "        }\n",
    "        pd.DataFrame([row], columns=summary_cols).to_csv(output_summary_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        # BEST PARAMS: keep + ADD best_windows_any for ALL classes\n",
    "        def median_or_none(arr):\n",
    "            arr = list(arr)\n",
    "            return _json_safe(float(np.median(arr)) if arr else None)\n",
    "\n",
    "        best_windows_any = {\n",
    "            \"sigma_peak_bins\": {\n",
    "                \"blue\":  {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"blue\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"blue\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"ark\":   {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"ark\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"ark\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"print\": {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"print\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"print\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"open\":  {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"open\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"open\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"global\":{\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"global\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_pre[\"global\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"intra\": {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_intra[\"intra\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_intra[\"intra\"][\"neg\"], step=sigma_bin_step)},\n",
    "                \"post\":  {\"pos\": stitch_numeric_bin_intervals_from_any(sigma_bins_post[\"post\"][\"pos\"], step=sigma_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(sigma_bins_post[\"post\"][\"neg\"], step=sigma_bin_step)},\n",
    "            },\n",
    "            \"bench_peak_bins\": {\n",
    "                \"blue\":  {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"blue\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"blue\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"ark\":   {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"ark\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"ark\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"print\": {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"print\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"print\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"open\":  {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"open\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"open\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"global\":{\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"global\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_pre[\"global\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"intra\": {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_intra[\"intra\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_intra[\"intra\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "                \"post\":  {\"pos\": stitch_numeric_bin_intervals_from_any(bench_bins_post[\"post\"][\"peak\"][\"pos\"], step=bench_bin_step),\n",
    "                          \"neg\": stitch_numeric_bin_intervals_from_any(bench_bins_post[\"post\"][\"peak\"][\"neg\"], step=bench_bin_step)},\n",
    "            },\n",
    "            \"time_start_bands\": {\n",
    "                \"blue\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"blue\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"blue\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"ark\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"ark\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"ark\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"print\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"print\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"print\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"open\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"open\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"open\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"global\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"global\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_pre_by_class_sign[\"global\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"intra\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_intra_by_class_sign[\"intra\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_intra_by_class_sign[\"intra\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "                \"post\": {\n",
    "                    \"pos\": stitch_timeband_intervals_from_any(timebands_post_by_class_sign[\"post\"][\"start\"][\"pos\"]),\n",
    "                    \"neg\": stitch_timeband_intervals_from_any(timebands_post_by_class_sign[\"post\"][\"start\"][\"neg\"]),\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "\n",
    "        best = {\n",
    "            \"ticker\": cur_ticker,\n",
    "            \"bench\": bench_name_seen,\n",
    "            \"static\": {\"corr\": _json_safe(corr_static), \"beta\": _json_safe(beta_static), \"sigma\": _json_safe(sigma_static)},\n",
    "\n",
    "            \"dev_print_last5_median\": {\n",
    "                \"pos\": median_or_none(last5_print_days_pos),\n",
    "                \"neg\": median_or_none(last5_print_days_neg),\n",
    "            },\n",
    "\n",
    "            \"totals\": {\n",
    "                \"events_total\": int(events_total),\n",
    "                \"pre_total\": int(events_pre_total),\n",
    "                \"intra_total\": int(events_intra_total),\n",
    "                \"post_total\": int(events_post_total),\n",
    "            },\n",
    "\n",
    "            \"ratings\": {\n",
    "                \"blue\": _json_safe(blue_r[\"rate_any\"]),\n",
    "                \"ark\": _json_safe(ark_r[\"rate_any\"]),\n",
    "                \"print\": _json_safe(pr_r[\"rate_any\"]),\n",
    "                \"open\": _json_safe(op_r[\"rate_any\"]),\n",
    "                \"intra\": _json_safe(intra_r[\"rate_any\"]),\n",
    "                \"post\": _json_safe(post_r[\"rate_any\"]),\n",
    "                \"global\": _json_safe(gl_r[\"rate_any\"]),\n",
    "            },\n",
    "\n",
    "            \"hard_soft_share\": {\n",
    "                \"blue\":  {\"hard\": int(blue_r[\"hard\"]), \"soft\": int(blue_r[\"soft\"]), \"hard_share\": _json_safe(blue_r[\"hard_share_in_norm\"])},\n",
    "                \"ark\":   {\"hard\": int(ark_r[\"hard\"]),  \"soft\": int(ark_r[\"soft\"]),  \"hard_share\": _json_safe(ark_r[\"hard_share_in_norm\"])},\n",
    "                \"print\": {\"hard\": int(pr_r[\"hard\"]),   \"soft\": int(pr_r[\"soft\"]),   \"hard_share\": _json_safe(pr_r[\"hard_share_in_norm\"])},\n",
    "                \"open\":  {\"hard\": int(op_r[\"hard\"]),   \"soft\": int(op_r[\"soft\"]),   \"hard_share\": _json_safe(op_r[\"hard_share_in_norm\"])},\n",
    "                \"intra\": {\"hard\": int(intra_r[\"hard\"]), \"soft\": int(intra_r[\"soft\"]), \"hard_share\": _json_safe(intra_r[\"hard_share_in_norm\"])},\n",
    "                \"post\":  {\"hard\": int(post_r[\"hard\"]),  \"soft\": int(post_r[\"soft\"]),  \"hard_share\": _json_safe(post_r[\"hard_share_in_norm\"])},\n",
    "                \"global\":{\"hard\": int(gl_r[\"hard\"]),   \"soft\": int(gl_r[\"soft\"]),   \"hard_share\": _json_safe(gl_r[\"hard_share_in_norm\"])},\n",
    "            },\n",
    "\n",
    "            \"avg_hard_delay_sec\": {\n",
    "                \"blue\": _json_safe(avg_hard_delay(\"blue\")),\n",
    "                \"ark\": _json_safe(avg_hard_delay(\"ark\")),\n",
    "                \"print\": _json_safe(avg_hard_delay(\"print\")),\n",
    "                \"open\": _json_safe(avg_hard_delay(\"open\")),\n",
    "                \"intra\": _json_safe(avg_hard_delay(\"intra\")),\n",
    "                \"post\": _json_safe(avg_hard_delay(\"post\")),\n",
    "                \"global\": _json_safe(avg_hard_delay(\"global\")),\n",
    "            },\n",
    "\n",
    "            \"best_windows_any\": {\n",
    "                \"rule\": {\"min_rate\": 0.60, \"min_total\": 4, \"rate\": \"(hard+soft)/total\"},\n",
    "                \"stitched\": best_windows_any,\n",
    "            },\n",
    "\n",
    "            \"params\": {\"dev_thr\": float(dev_thr), \"norm_thr\": float(norm_thr), \"soft_ratio\": float(soft_ratio)},\n",
    "        }\n",
    "\n",
    "        best_params_f.write(json.dumps(best, ensure_ascii=False) + \"\\n\")\n",
    "        reset_ticker_state()\n",
    "        \n",
    "    # ---------------- processing chunks ----------------\n",
    "    def process_chunk(chunk: \"pd.DataFrame\", ci: int):\n",
    "        nonlocal cur_ticker, cur_day\n",
    "        nonlocal bench_name_seen, static_triplet_set, corr_static, beta_static, sigma_static\n",
    "        nonlocal pre_active, intra_active, post_active\n",
    "\n",
    "        req = {\"ticker\", \"date\", \"dt\", \"dev_sig\"}\n",
    "        if not req.issubset(chunk.columns):\n",
    "            raise KeyError(f\"Input must contain columns: {sorted(req)}\")\n",
    "\n",
    "        # If not sorted — make it sorted once (vectorized), no per-row parse_dt\n",
    "        if not assume_sorted:\n",
    "            chunk[\"dt\"] = pd.to_datetime(chunk[\"dt\"], errors=\"coerce\", utc=True)\n",
    "            chunk.sort_values([\"ticker\", \"date\", \"dt\"], inplace=True)\n",
    "\n",
    "        def col(name):\n",
    "            return chunk[name] if name in chunk.columns else pd.Series(np.nan, index=chunk.index)\n",
    "\n",
    "        s_ticker = col(\"ticker\")\n",
    "        s_date   = col(\"date\")\n",
    "\n",
    "        # ✅ Vectorized datetime + numeric parse\n",
    "        s_dt_ts  = pd.to_datetime(col(\"dt\"), errors=\"coerce\", utc=True)  # tz-aware Timestamp (UTC)\n",
    "        s_dev    = pd.to_numeric(col(\"dev_sig\"), errors=\"coerce\")\n",
    "\n",
    "        s_bench_name = col(\"bench\")\n",
    "        s_corr  = col(\"corr\")\n",
    "        s_beta  = col(\"beta\")\n",
    "        s_sigma = col(\"sigma\")\n",
    "\n",
    "        s_bench_num = (\n",
    "            pd.to_numeric(col(BENCH_NUM_FIELD), errors=\"coerce\")\n",
    "            if BENCH_NUM_FIELD in chunk.columns\n",
    "            else pd.Series(np.nan, index=chunk.index)\n",
    "        )\n",
    "        s_stock_pct = (\n",
    "            pd.to_numeric(col(STOCK_NUM_FIELD), errors=\"coerce\")\n",
    "            if STOCK_NUM_FIELD in chunk.columns\n",
    "            else pd.Series(np.nan, index=chunk.index)\n",
    "        )\n",
    "\n",
    "        # ✅ Build a fast mask upfront (drop bad dt/dev and ignored windows) BEFORE loop\n",
    "        dt_ok  = s_dt_ts.notna()\n",
    "        dev_ok = np.isfinite(s_dev.to_numpy(dtype=\"float64\", copy=False))\n",
    "        mask = (dt_ok.to_numpy(copy=False) & dev_ok)\n",
    "\n",
    "        if mask.any():\n",
    "            # ignored windows mask using hour/min (vectorized)\n",
    "            dt2 = s_dt_ts[mask]\n",
    "            h = dt2.dt.hour.to_numpy(dtype=\"int16\", copy=False)\n",
    "            m = dt2.dt.minute.to_numpy(dtype=\"int16\", copy=False)\n",
    "\n",
    "            # vectorized ignored windows:\n",
    "            # IGNORE_WINDOWS = [((3, 58), (4, 5)), ((7, 58), (8, 5))]\n",
    "            # condition: a <= (h,m) <= b for any window\n",
    "            def _in_win(h, m, a, b):\n",
    "                ah, am = a\n",
    "                bh, bm = b\n",
    "                return ((h > ah) | ((h == ah) & (m >= am))) & ((h < bh) | ((h == bh) & (m <= bm)))\n",
    "\n",
    "            ig = np.zeros_like(h, dtype=bool)\n",
    "            for a, b in IGNORE_WINDOWS:\n",
    "                ig |= _in_win(h, m, a, b)\n",
    "\n",
    "            # apply back to global mask\n",
    "            idx_mask = np.flatnonzero(mask)\n",
    "            mask[idx_mask] = ~ig\n",
    "\n",
    "        # nothing useful in chunk\n",
    "        if not mask.any():\n",
    "            return\n",
    "\n",
    "        # ✅ set bench_name_seen once per chunk (first non-null)\n",
    "        if bench_name_seen is None and \"bench\" in chunk.columns:\n",
    "            bn = s_bench_name[mask]\n",
    "            if len(bn) > 0:\n",
    "                first_bn = bn.dropna()\n",
    "                if len(first_bn) > 0:\n",
    "                    bench_name_seen = first_bn.iloc[0]\n",
    "\n",
    "        # ✅ set static triplet once per chunk (first row with corr/beta/sigma present)\n",
    "        if not static_triplet_set and all(x in chunk.columns for x in (\"corr\", \"beta\", \"sigma\")):\n",
    "            cc = s_corr[mask]\n",
    "            bb = s_beta[mask]\n",
    "            ss = s_sigma[mask]\n",
    "            ok = (cc.notna() & bb.notna() & ss.notna())\n",
    "            if ok.any():\n",
    "                j = ok.idxmax()  # first True index\n",
    "                corr_static, beta_static, sigma_static = cc.loc[j], bb.loc[j], ss.loc[j]\n",
    "                static_triplet_set = True\n",
    "\n",
    "        # ✅ Build compact arrays (keep original order)\n",
    "        # Convert to python datetime ONLY for the filtered rows (cheap vs per-row parse_dt)\n",
    "        dt_py = s_dt_ts[mask].dt.to_pydatetime()  # ndarray of datetime (tz-aware)\n",
    "        tk_arr = s_ticker[mask].to_numpy(copy=False)\n",
    "        ds_arr = s_date[mask].to_numpy(copy=False)\n",
    "        dev_arr = s_dev[mask].to_numpy(dtype=\"float64\", copy=False)\n",
    "        bench_arr = s_bench_num[mask].to_numpy(dtype=\"float64\", copy=False)\n",
    "        stock_arr = s_stock_pct[mask].to_numpy(dtype=\"float64\", copy=False)\n",
    "\n",
    "        # Precompute hour/min for fast tuple t\n",
    "        dt2 = s_dt_ts[mask]\n",
    "        h_arr = dt2.dt.hour.to_numpy(dtype=\"int16\", copy=False)\n",
    "        m_arr = dt2.dt.minute.to_numpy(dtype=\"int16\", copy=False)\n",
    "\n",
    "        n = len(dev_arr)\n",
    "        for i in range(n):\n",
    "            tk = tk_arr[i]\n",
    "            ds = ds_arr[i]\n",
    "            dt_now = dt_py[i]         # datetime with tzinfo\n",
    "            dev_now = dev_arr[i]      # float\n",
    "            bench_num = bench_arr[i]  # float (may be nan)\n",
    "            stock_pct = stock_arr[i]  # float (may be nan)\n",
    "\n",
    "            # cheap time tuple\n",
    "            t = (int(h_arr[i]), int(m_arr[i]))\n",
    "\n",
    "            # ticker boundary\n",
    "            if cur_ticker is not None and tk != cur_ticker:\n",
    "                flush_current_ticker()\n",
    "                cur_ticker, cur_day = tk, ds\n",
    "                on_new_day()\n",
    "\n",
    "            if cur_ticker is None:\n",
    "                cur_ticker, cur_day = tk, ds\n",
    "                on_new_day()\n",
    "\n",
    "            # day boundary\n",
    "            if ds != cur_day:\n",
    "                cur_day = ds\n",
    "                on_new_day()\n",
    "\n",
    "            # finalize PRE after OPEN window\n",
    "            if pre_active and (t > OPEN_TO):\n",
    "                finalize_pre_event(reason=\"passed_open_window\")\n",
    "\n",
    "            # start PRE in BLUE or ARK window\n",
    "            if (not pre_active) and (in_range(t, BLUE_FROM, BLUE_TO) or in_range(t, ARK_FROM, ARK_TO)):\n",
    "                if abs(float(dev_now)) >= dev_thr:\n",
    "                    start_pre_event(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "            if pre_active:\n",
    "                pre_process_tick(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "            # finalize INTRA after window\n",
    "            if intra_active and (t > INTRA_TO):\n",
    "                finalize_intra_event(reason=\"passed_intra_window\")\n",
    "\n",
    "            # start INTRA inside 10:00-12:00\n",
    "            if (not intra_active) and in_range(t, INTRA_FROM, INTRA_TO):\n",
    "                if abs(float(dev_now)) >= dev_thr:\n",
    "                    start_intra_event(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "            if intra_active and in_range(t, INTRA_FROM, INTRA_TO):\n",
    "                intra_process_tick(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "            # finalize POST after window\n",
    "            if post_active and (t > POST_TO):\n",
    "                finalize_post_event(reason=\"passed_post_window\")\n",
    "\n",
    "            # start POST inside 16:01-19:59\n",
    "            if (not post_active) and in_range(t, POST_FROM, POST_TO):\n",
    "                if abs(float(dev_now)) >= dev_thr:\n",
    "                    start_post_event(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "            if post_active and in_range(t, POST_FROM, POST_TO):\n",
    "                post_process_tick(dt_now, dev_now, stock_pct, bench_num)\n",
    "\n",
    "\n",
    "    # ---------------- main read loop ----------------\n",
    "    t0 = time.time()\n",
    "    total_rows = 0\n",
    "    last_rows = 0\n",
    "    last_ts = t0\n",
    "\n",
    "    is_parquet = str(input_path).lower().endswith((\".parquet\", \".pq\", \".parq\"))\n",
    "    print(\n",
    "        f\"▶️ START v12 exporter+BLUE+POST file={input_path} parquet={is_parquet} \"\n",
    "        f\"dev_thr={dev_thr} norm_thr={norm_thr} soft_ratio={soft_ratio} min_events={min_events_per_ticker}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if is_parquet and parquet_use_pyarrow:\n",
    "            import pyarrow.parquet as pq\n",
    "            pf = pq.ParquetFile(input_path)\n",
    "\n",
    "            wanted = [\"ticker\", \"date\", \"dt\", \"dev_sig\", \"bench\", \"corr\", \"beta\", \"sigma\", STOCK_NUM_FIELD, BENCH_NUM_FIELD]\n",
    "            cols = [c for c in wanted if c in pf.schema.names]\n",
    "\n",
    "            for ci in range(pf.num_row_groups):\n",
    "                chunk = pf.read_row_group(ci, columns=cols).to_pandas()\n",
    "                process_chunk(chunk, ci + 1)\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                if (ci + 1) % log_every_n_chunks == 0:\n",
    "                    now = time.time()\n",
    "                    rps = (total_rows - last_rows) / max(now - last_ts, 1e-6)\n",
    "                    print(f\"[rg {ci+1:>4}/{pf.num_row_groups}] rows={total_rows:,} speed={rps:,.0f}/s elapsed={now-t0:,.1f}s\")\n",
    "                    last_rows, last_ts = total_rows, now\n",
    "\n",
    "                # cheaper cleanup (avoid gc.collect each chunk)\n",
    "                del chunk\n",
    "                if (ci + 1) % max(10, log_every_n_chunks * 2) == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "        elif not is_parquet:\n",
    "            reader = pd.read_csv(input_path, compression=\"infer\", low_memory=False, chunksize=csv_chunksize)\n",
    "            for ci, chunk in enumerate(reader, 1):\n",
    "                process_chunk(chunk, ci)\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                if ci % log_every_n_chunks == 0:\n",
    "                    now = time.time()\n",
    "                    rps =qar = (total_rows - last_rows) / max(now - last_ts, 1e-6)\n",
    "                    print(f\"[chunk {ci:>5}] rows={total_rows:,} speed={rps:,.0f}/s elapsed={now-t0:,.1f}s\")\n",
    "                    last_rows, last_ts = total_rows, now\n",
    "\n",
    "                del chunk\n",
    "                if ci % max(10, log_every_n_chunks * 2) == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "        else:\n",
    "            df = pd.read_parquet(input_path)\n",
    "            step = 1_000_000\n",
    "            for ci, start in enumerate(range(0, len(df), step), 1):\n",
    "                chunk = df.iloc[start:start + step]\n",
    "                process_chunk(chunk, ci)\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                if ci % log_every_n_chunks == 0:\n",
    "                    now = time.time()\n",
    "                    rps = (total_rows - last_rows) / max(now - last_ts, 1e-6)\n",
    "                    print(f\"[chunk {ci:>5}] rows={total_rows:,} speed={rps:,.0f}/s elapsed={now-t0:,.1f}s\")\n",
    "                    last_rows, last_ts = total_rows, now\n",
    "\n",
    "                del chunk\n",
    "                if ci % max(10, log_every_n_chunks * 2) == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "        flush_current_ticker()\n",
    "        print(f\"🏁 DONE rows={total_rows:,} -> onefile={output_onefile_jsonl} summary={output_summary_csv} best_params={output_best_params_jsonl}\")\n",
    "\n",
    "    finally:\n",
    "        onefile_f.close()\n",
    "        best_params_f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea8ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devsig_stream_stats_v12_exporter(\n",
    "#     input_path=\"ARBITRAGE/final_filtered.parquet\",\n",
    "#     output_onefile_jsonl=\"ARBITRAGE/onefile.jsonl\",\n",
    "#     output_summary_csv=\"ARBITRAGE/summary.csv\",\n",
    "#     output_best_params_jsonl=\"ARBITRAGE/best_params.jsonl\",\n",
    "\n",
    "#     dev_thr=0.30,\n",
    "#     norm_thr=0.10,\n",
    "#     soft_ratio=3.0,\n",
    "\n",
    "#     include_events_pre=False,\n",
    "#     include_events_intra=False,\n",
    "#     max_events_per_ticker=500,\n",
    "\n",
    "#     min_events_per_ticker=10,\n",
    "\n",
    "#     start_band_minutes=30,\n",
    "#     norm_band_minutes=30,\n",
    "\n",
    "#     sigma_bin_min=0.2,\n",
    "#     sigma_bin_max=2.7,\n",
    "#     sigma_bin_step=0.1,\n",
    "\n",
    "#     bench_bin_min=-3.0,\n",
    "#     bench_bin_max=3.0,\n",
    "#     bench_bin_step=0.1,\n",
    "\n",
    "#     open_series_downsample_seconds=60,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a1c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading FINAL parquet: C:\\datum-api-examples-main\\OriON\\CRACEN\\final.parquet\n",
      "Sorting by ticker + dt (ascending)...\n",
      "Sorted parquet written: C:\\datum-api-examples-main\\OriON\\CRACEN\\final_sorted_tmp.parquet\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'devsig_stream_stats_v12_exporter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m\n\u001b[0;32m     83\u001b[0m df\u001b[38;5;241m.\u001b[39mto_parquet(sorted_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorted parquet written:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sorted_path)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mdevsig_stream_stats_v12_exporter\u001b[49m(\n\u001b[0;32m     88\u001b[0m     input_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(sorted_path),\n\u001b[0;32m     89\u001b[0m     output_onefile_jsonl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(OUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monefile.jsonl.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     90\u001b[0m     output_best_params_jsonl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(OUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_params.jsonl.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     91\u001b[0m     output_summary_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(OUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m     dev_thr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.30\u001b[39m,\n\u001b[0;32m     94\u001b[0m     norm_thr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m,\n\u001b[0;32m     95\u001b[0m     soft_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m,\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m     include_events_pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m     include_events_intra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     99\u001b[0m     max_events_per_ticker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m    100\u001b[0m     min_events_per_ticker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m     start_band_minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    103\u001b[0m     norm_band_minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m     sigma_bin_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m    106\u001b[0m     sigma_bin_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.7\u001b[39m,\n\u001b[0;32m    107\u001b[0m     sigma_bin_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m     bench_bin_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.0\u001b[39m,\n\u001b[0;32m    110\u001b[0m     bench_bin_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m,\n\u001b[0;32m    111\u001b[0m     bench_bin_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m     open_series_downsample_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m    114\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'devsig_stream_stats_v12_exporter' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _resolve_orion_paths(strategy_code: str):\n",
    "    final_env = os.environ.get(\"FINAL_PARQUET_PATH\")\n",
    "    sig_env   = os.environ.get(\"SIGNALS_DIR\")\n",
    "    orion_env = os.environ.get(\"ORION_HOME\")\n",
    "\n",
    "    final_path = Path(final_env).expanduser().resolve() if final_env else None\n",
    "    signals_base = Path(sig_env).expanduser().resolve() if sig_env else None\n",
    "\n",
    "    if (final_path is None or signals_base is None) and orion_env:\n",
    "        orion_home = Path(orion_env).expanduser().resolve()\n",
    "        if final_path is None:\n",
    "            final_path = (orion_home / \"CRACEN\" / \"final.parquet\").resolve()\n",
    "        if signals_base is None:\n",
    "            signals_base = (orion_home / \"signals\").resolve()\n",
    "\n",
    "    if final_path is None or signals_base is None:\n",
    "        here = Path.cwd().resolve()\n",
    "        orion_home = None\n",
    "        for parent in [here] + list(here.parents):\n",
    "            if parent.name.lower() == \"orion\":\n",
    "                orion_home = parent\n",
    "                break\n",
    "            cand = parent / \"OriON\"\n",
    "            if cand.exists() and cand.is_dir():\n",
    "                orion_home = cand.resolve()\n",
    "                break\n",
    "\n",
    "        if orion_home is None:\n",
    "            raise RuntimeError(\"Cannot locate OriON. Set ORION_HOME (recommended).\")\n",
    "\n",
    "        if final_path is None:\n",
    "            final_path = (orion_home / \"CRACEN\" / \"final.parquet\").resolve()\n",
    "        if signals_base is None:\n",
    "            signals_base = (orion_home / \"signals\").resolve()\n",
    "\n",
    "    out_dir = (signals_base / strategy_code.lower()).resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not final_path.exists():\n",
    "        raise FileNotFoundError(f\"FINAL parquet not found: {final_path}\")\n",
    "\n",
    "    return final_path, out_dir\n",
    "\n",
    "\n",
    "def _pick_datetime_col(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"datetime\", \"dt\", \"timestamp\", \"time\", \"ts\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Cannot find datetime column. Expected one of {candidates}, got columns: {list(df.columns)[:40]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "FINAL_PATH, OUT_DIR = _resolve_orion_paths(\"arbitrage\")\n",
    "\n",
    "print(\"Reading FINAL parquet:\", FINAL_PATH)\n",
    "df = pd.read_parquet(FINAL_PATH)\n",
    "\n",
    "if \"ticker\" not in df.columns:\n",
    "    raise KeyError(f\"FINAL parquet missing 'ticker' column. Columns: {list(df.columns)[:40]}\")\n",
    "\n",
    "dt_col = _pick_datetime_col(df)\n",
    "\n",
    "print(f\"Sorting by ticker + {dt_col} (ascending)...\")\n",
    "df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "if df[dt_col].isna().any():\n",
    "    bad = int(df[dt_col].isna().sum())\n",
    "    raise ValueError(f\"{bad} rows have invalid {dt_col} (NaT) after to_datetime().\")\n",
    "\n",
    "df = df.sort_values([\"ticker\", dt_col], ascending=[True, True])\n",
    "\n",
    "sorted_path = FINAL_PATH.with_name(FINAL_PATH.stem + \"_sorted_tmp.parquet\")\n",
    "df.to_parquet(sorted_path, index=False)\n",
    "\n",
    "print(\"Sorted parquet written:\", sorted_path)\n",
    "\n",
    "devsig_stream_stats_v12_exporter(\n",
    "    input_path=str(sorted_path),\n",
    "    output_onefile_jsonl=str(OUT_DIR / \"onefile.jsonl.gz\"),\n",
    "    output_best_params_jsonl=str(OUT_DIR / \"best_params.jsonl.gz\"),\n",
    "    output_summary_csv=str(OUT_DIR / \"summary.csv\"),\n",
    "\n",
    "    dev_thr=0.30,\n",
    "    norm_thr=0.10,\n",
    "    soft_ratio=3.0,\n",
    "\n",
    "    include_events_pre=False,\n",
    "    include_events_intra=False,\n",
    "    max_events_per_ticker=500,\n",
    "    min_events_per_ticker=10,\n",
    "\n",
    "    start_band_minutes=30,\n",
    "    norm_band_minutes=30,\n",
    "\n",
    "    sigma_bin_min=0.2,\n",
    "    sigma_bin_max=2.7,\n",
    "    sigma_bin_step=0.1,\n",
    "\n",
    "    bench_bin_min=-3.0,\n",
    "    bench_bin_max=3.0,\n",
    "    bench_bin_step=0.1,\n",
    "\n",
    "    open_series_downsample_seconds=60,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
